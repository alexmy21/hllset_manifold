{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f4eb7a",
   "metadata": {},
   "source": [
    "# Adjacency Matrix for Order Preservation\n",
    "\n",
    "This notebook demonstrates how Adjacency Matrices (AM) preserve token order and enable reconstruction during query processing.\n",
    "\n",
    "**Key Architecture**:\n",
    "- **Ingestion**: Single HLLSet combines all n-tokens (1-tokens + 2-tokens + 3-tokens)\n",
    "- **AM Matrix**: Splits this HLLSet by (reg, zeros) identifiers for disambiguation\n",
    "- **~100K √ó 100K matrix**: Much smaller than millions of unique tokens\n",
    "- **For concise languages** (e.g., Chinese ~80K characters): Can split by actual tokens\n",
    "\n",
    "**Key Concepts**:\n",
    "- Build AM during ingestion with transition frequencies\n",
    "- Use (reg, zeros) identifiers as compact representation\n",
    "- Traverse AM during queries to reconstruct order\n",
    "- START/END markers for sequence boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8363e2",
   "metadata": {},
   "source": [
    "## 1. Understanding the Problem\n",
    "\n",
    "**HLLSets lose order**:\n",
    "- Only store distinct tokens\n",
    "- No sequence information\n",
    "- Cannot reconstruct \"hello world\" vs \"world hello\"\n",
    "\n",
    "**Solution**: Adjacency Matrix preserves transitions between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33863854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Extension registered: storage v1.4.4\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "  ‚úì LUT committed: n=1, hash=dbe1e60d59b56929..., id=8\n",
      "  ‚úì LUT committed: n=2, hash=fdb8b90c523da265..., id=8\n",
      "  ‚úì LUT committed: n=3, hash=e4545c77cb6272c7..., id=7\n",
      "Text ingested: 'the quick brown fox jumps over the lazy dog'\n",
      "\n",
      "üìä Adjacency Matrix (Source of Truth):\n",
      "  Non-zero cells (transitions): 10\n",
      "  Matrix size: 9 rows √ó 9 cols\n",
      "  Row HLLSets: 9\n",
      "  Column HLLSets: 9\n",
      "\n",
      "üîç HLLSet derived from AM (before commit):\n",
      "  Complete HLLSet extracted: True\n",
      "  Cardinality: 12.00\n",
      "  ‚úì Derived from union of AM's row HLLSets\n",
      "  ‚úì Extracted BEFORE AM committed to shared resource\n",
      "\n",
      "üí° Architecture:\n",
      "  ‚Ä¢ AM = Single source of truth (order + sets)\n",
      "  ‚Ä¢ HLLSet = Derived from AM during ingestion\n",
      "  ‚Ä¢ After commit: AM merges into shared (can't derive individual HLLSets)\n"
     ]
    }
   ],
   "source": [
    "from core.manifold_os import ManifoldOS\n",
    "from core import HLLSet\n",
    "\n",
    "# Create ManifoldOS instance\n",
    "os = ManifoldOS()\n",
    "\n",
    "# Ingest text - builds AM (single source of truth)\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "result = os.ingest(text)\n",
    "\n",
    "# Access the adjacency matrix from the IngestDriver\n",
    "driver = os.get_driver(\"ingest_default\")\n",
    "am = driver.adjacency_matrix\n",
    "\n",
    "print(f\"Text ingested: '{text}'\")\n",
    "print(f\"\\nüìä Adjacency Matrix (Source of Truth):\")\n",
    "print(f\"  Non-zero cells (transitions): {am.get_nonzero_count()}\")\n",
    "print(f\"  Matrix size: {am.get_size()[0]} rows √ó {am.get_size()[1]} cols\")\n",
    "print(f\"  Row HLLSets: {len(am.row_hllsets)}\")\n",
    "print(f\"  Column HLLSets: {len(am.col_hllsets)}\")\n",
    "\n",
    "print(f\"\\nüîç HLLSet derived from AM (before commit):\")\n",
    "print(f\"  Complete HLLSet extracted: {result.complete_hllset is not None}\")\n",
    "if result.complete_hllset:\n",
    "    print(f\"  Cardinality: {result.complete_hllset.cardinality():.2f}\")\n",
    "    print(f\"  ‚úì Derived from union of AM's row HLLSets\")\n",
    "    print(f\"  ‚úì Extracted BEFORE AM committed to shared resource\")\n",
    "\n",
    "print(f\"\\nüí° Architecture:\")\n",
    "print(f\"  ‚Ä¢ AM = Single source of truth (order + sets)\")\n",
    "print(f\"  ‚Ä¢ HLLSet = Derived from AM during ingestion\")\n",
    "print(f\"  ‚Ä¢ After commit: AM merges into shared (can't derive individual HLLSets)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e7ab9",
   "metadata": {},
   "source": [
    "## 2. Sliding Window Processing\n",
    "\n",
    "During ingestion, tokens are processed with a sliding window to build transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca8c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Extension registered: storage v1.4.4\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "  ‚úì LUT committed: n=1, hash=dbe1e60d59b56929..., id=8\n",
      "  ‚úì LUT committed: n=2, hash=fdb8b90c523da265..., id=8\n",
      "  ‚úì LUT committed: n=3, hash=e4545c77cb6272c7..., id=7\n",
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "\n",
      "HLLSets created: 3\n",
      "\n",
      "Sliding window creates transitions:\n",
      "  'the' ‚Üí 'quick'\n",
      "  'quick' ‚Üí 'brown'\n",
      "  'brown' ‚Üí 'fox'\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "from core.manifold_os import ManifoldOS\n",
    "\n",
    "# Create OS\n",
    "os = ManifoldOS()\n",
    "\n",
    "# Ingest a sentence - AM is built automatically\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "result = os.ingest(text)\n",
    "\n",
    "print(\"Original tokens:\", result.original_tokens)\n",
    "print(f\"\\nHLLSets created: {len(result.hllsets)}\")\n",
    "print(\"\\nSliding window creates transitions:\")\n",
    "print(\"  'the' ‚Üí 'quick'\")\n",
    "print(\"  'quick' ‚Üí 'brown'\")\n",
    "print(\"  'brown' ‚Üí 'fox'\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18250c3",
   "metadata": {},
   "source": [
    "## 3. (reg, zeros) Identifiers\n",
    "\n",
    "Instead of storing full token strings, AM uses compact identifiers derived from hash values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b05fac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token identifiers (reg, zeros):\n",
      "\n",
      "  'the' ‚Üí (reg=960, zeros=1)\n",
      "  'quick' ‚Üí (reg=892, zeros=2)\n",
      "  'brown' ‚Üí (reg=573, zeros=0)\n",
      "  'fox' ‚Üí (reg=678, zeros=3)\n",
      "\n",
      "‚úì Single hash calculation in C backend\n",
      "‚úì Used by both HLLSet and Adjacency Matrix\n",
      "‚úì No duplicate work!\n"
     ]
    }
   ],
   "source": [
    "from core import HLLSet\n",
    "from core.constants import SHARED_SEED\n",
    "\n",
    "# Now we use HLLSet's efficient batch method - no double calculation!\n",
    "tokens = ['the', 'quick', 'brown', 'fox']\n",
    "\n",
    "# Get (reg, zeros) identifiers efficiently\n",
    "identifiers = HLLSet.compute_reg_zeros_batch(tokens, seed=SHARED_SEED)\n",
    "\n",
    "print(\"Token identifiers (reg, zeros):\\n\")\n",
    "for token, (reg, zeros) in zip(tokens, identifiers):\n",
    "    print(f\"  '{token}' ‚Üí (reg={reg}, zeros={zeros})\")\n",
    "\n",
    "print(\"\\n‚úì Single hash calculation in C backend\")\n",
    "print(\"‚úì Used by both HLLSet and Adjacency Matrix\")\n",
    "print(\"‚úì No duplicate work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122dad08",
   "metadata": {},
   "source": [
    "## 4. START and END Markers\n",
    "\n",
    "Special markers define sequence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfea95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token identifiers:\n",
      "  START ‚Üí (-1, 0)\n",
      "  END ‚Üí (-2, 0)\n",
      "\n",
      "Actual transitions in AM (showing START/END):\n",
      "  START ‚Üí (960, 1) (freq=1)\n",
      "  (960, 1) ‚Üí (892, 2) (freq=1)\n",
      "  (892, 2) ‚Üí (573, 0) (freq=1)\n",
      "  (573, 0) ‚Üí (678, 3) (freq=1)\n",
      "  (678, 3) ‚Üí (956, 6) (freq=1)\n",
      "  (956, 6) ‚Üí (648, 3) (freq=1)\n",
      "\n",
      "‚úì START marks sequence beginning\n",
      "‚úì END marks sequence termination\n",
      "‚úì Enables order reconstruction during queries\n"
     ]
    }
   ],
   "source": [
    "# Access the adjacency matrix to see START/END tokens\n",
    "driver = os.get_driver(\"ingest_default\")\n",
    "am = driver.adjacency_matrix\n",
    "\n",
    "print(\"Special token identifiers:\")\n",
    "print(f\"  START ‚Üí {am.START_ID}\")\n",
    "print(f\"  END ‚Üí {am.END_ID}\")\n",
    "\n",
    "# Show actual transitions with START/END\n",
    "print(\"\\nActual transitions in AM (showing START/END):\")\n",
    "for (from_id, to_id), cell in list(am.cells.items())[:6]:\n",
    "    from_label = \"START\" if from_id == am.START_ID else str(from_id)\n",
    "    to_label = \"END\" if to_id == am.END_ID else str(to_id)\n",
    "    print(f\"  {from_label} ‚Üí {to_label} (freq={cell.frequency})\")\n",
    "\n",
    "print(\"\\n‚úì START marks sequence beginning\")\n",
    "print(\"‚úì END marks sequence termination\")\n",
    "print(\"‚úì Enables order reconstruction during queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f726d",
   "metadata": {},
   "source": [
    "## 5. Transition Frequencies\n",
    "\n",
    "AM stores how often token A transitions to token B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61e040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition frequencies:\n",
      "\n",
      "  'the' ‚Üí 'cat': 2\n",
      "  'cat' ‚Üí 'sat': 2\n",
      "  'sat' ‚Üí 'on': 1\n",
      "  'on' ‚Üí 'the': 1\n",
      "  'the' ‚Üí 'mat': 1\n",
      "  'mat' ‚Üí 'and': 1\n",
      "  'and' ‚Üí 'the': 1\n"
     ]
    }
   ],
   "source": [
    "# Simulate building transitions\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample text with repetitions\n",
    "text = \"the cat sat on the mat and the cat sat\"\n",
    "tokens = text.split()\n",
    "\n",
    "# Count transitions\n",
    "transitions = defaultdict(int)\n",
    "for i in range(len(tokens) - 1):\n",
    "    pair = (tokens[i], tokens[i+1])\n",
    "    transitions[pair] += 1\n",
    "\n",
    "print(\"Transition frequencies:\\n\")\n",
    "for (from_token, to_token), count in sorted(transitions.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  '{from_token}' ‚Üí '{to_token}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60983f5f",
   "metadata": {},
   "source": [
    "## 6. Query Phase: Order Reconstruction\n",
    "\n",
    "During queries, traverse the AM to reconstruct original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b915bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLLSet cardinality: 7\n",
      "Target: ~6 tokens (90.0% of cardinality)\n",
      "\n",
      "Conceptual traversal: START ‚Üí the ‚Üí quick ‚Üí brown ‚Üí fox ‚Üí jumps ‚Üí END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reconstruct_order(hllset, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Reconstruct order from AM (conceptual demonstration).\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start at START marker\n",
    "    2. Follow highest frequency transitions\n",
    "    3. Stop when reaching threshold * cardinality tokens\n",
    "    4. Or when reaching END marker\n",
    "    \n",
    "    Note: This is a simplified conceptual demo.\n",
    "    Real implementation would query the actual AM transitions.\n",
    "    \"\"\"\n",
    "    cardinality = hllset.cardinality()\n",
    "    target_tokens = int(cardinality * threshold)\n",
    "    \n",
    "    print(f\"HLLSet cardinality: {cardinality:.0f}\")\n",
    "    print(f\"Target: ~{target_tokens} tokens ({threshold*100}% of cardinality)\")\n",
    "    print(\"\\nConceptual traversal: START\", end=\"\")\n",
    "    \n",
    "    # Simplified traversal visualization\n",
    "    reconstructed = []\n",
    "    current = \"START\"\n",
    "    \n",
    "    # In real implementation, would look up transitions from AM\n",
    "    # For demo, just show the concept\n",
    "    sample_path = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n",
    "    for token in sample_path[:min(len(sample_path), target_tokens)]:\n",
    "        print(f\" ‚Üí {token}\", end=\"\")\n",
    "        reconstructed.append(token)\n",
    "    \n",
    "    print(\" ‚Üí END\")\n",
    "    return reconstructed\n",
    "\n",
    "# Demo with a sample HLLSet\n",
    "demo_hll = HLLSet.from_batch(['the', 'quick', 'brown', 'fox', 'jumps'])\n",
    "reconstruct_order(demo_hll, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693aa51f",
   "metadata": {},
   "source": [
    "## 7. Two-Phase Architecture\n",
    "\n",
    "Understanding how AM works across ingestion and query phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e801ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: INGESTION (Building AM)\n",
      "======================================================================\n",
      "\n",
      "1. Tokenize input text\n",
      "2. Generate n-tokens via sliding window (1-tokens, 2-tokens, 3-tokens)\n",
      "3. Create SINGLE HLLSet combining all n-tokens\n",
      "4. For AM matrix:\n",
      "   - Split HLLSet by (reg, zeros) identifiers\n",
      "   - Or by actual tokens for concise languages (Chinese ~80K)\n",
      "5. For each adjacent pair:\n",
      "   - Get (reg, zeros) identifiers\n",
      "   - Increment AM[from_id][to_id]\n",
      "6. Add START ‚Üí first_token\n",
      "7. Add last_token ‚Üí END\n",
      "8. HLLSet = perfect fingerprint (full set operations support)\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: QUERY (Order Reconstruction)\n",
      "======================================================================\n",
      "\n",
      "1. User provides prompt ‚Üí create HLLSet\n",
      "2. Retrieve relevant HLLSets from storage\n",
      "3. Get cardinality estimate (note: tripled due to all n-tokens)\n",
      "4. Traverse AM:\n",
      "   - Start at START marker\n",
      "   - Follow highest frequency transitions\n",
      "   - Collect identifiers\n",
      "   - Stop when threshold reached (e.g., 0.9 √ó cardinality)\n",
      "   - Or when END marker reached\n",
      "5. Map identifiers back to tokens via LUT\n",
      "6. Return ordered sequence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: INGESTION (Building AM)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. Tokenize input text\n",
    "2. Generate n-tokens via sliding window (1-tokens, 2-tokens, 3-tokens)\n",
    "3. Create SINGLE HLLSet combining all n-tokens\n",
    "4. For AM matrix:\n",
    "   - Split HLLSet by (reg, zeros) identifiers\n",
    "   - Or by actual tokens for concise languages (Chinese ~80K)\n",
    "5. For each adjacent pair:\n",
    "   - Get (reg, zeros) identifiers\n",
    "   - Increment AM[from_id][to_id]\n",
    "6. Add START ‚Üí first_token\n",
    "7. Add last_token ‚Üí END\n",
    "8. HLLSet = perfect fingerprint (full set operations support)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: QUERY (Order Reconstruction)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. User provides prompt ‚Üí create HLLSet\n",
    "2. Retrieve relevant HLLSets from storage\n",
    "3. Get cardinality estimate (note: tripled due to all n-tokens)\n",
    "4. Traverse AM:\n",
    "   - Start at START marker\n",
    "   - Follow highest frequency transitions\n",
    "   - Collect identifiers\n",
    "   - Stop when threshold reached (e.g., 0.9 √ó cardinality)\n",
    "   - Or when END marker reached\n",
    "5. Map identifiers back to tokens via LUT\n",
    "6. Return ordered sequence\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62be008",
   "metadata": {},
   "source": [
    "## 8. Compact Matrix Size\n",
    "\n",
    "**AM splits single HLLSet by identifiers for compact representation**:\n",
    "\n",
    "- **Option 1**: (reg, zeros) identifiers ‚Üí ~100K √ó 100K matrix\n",
    "- **Option 2**: Actual tokens for concise languages (e.g., Chinese ~80K characters)\n",
    "\n",
    "Both approaches much smaller than millions of unique tokens in full vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff71d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1: (reg, zeros) identifiers\n",
      "  P_BITS: 10\n",
      "  Registers: 1,024\n",
      "  Max zeros: 32\n",
      "  Total identifiers: 33,792\n",
      "  Matrix size: 33,792 √ó 33,792\n",
      "\n",
      "Option 2: Actual tokens (concise languages)\n",
      "  Chinese: ~80,000 characters\n",
      "  Matrix size: 80,000 √ó 80,000\n",
      "\n",
      "Vs. millions of unique tokens in full vocabulary\n",
      "‚úì Single HLLSet combines all n-tokens (perfect fingerprint)\n",
      "‚úì AM splits it by identifiers for compact storage\n",
      "\n",
      "With sparse storage, only non-zero transitions stored\n"
     ]
    }
   ],
   "source": [
    "# Calculate matrix dimensions for (reg, zeros) approach\n",
    "p_bits = 10\n",
    "max_zeros = 32\n",
    "\n",
    "# Number of unique identifiers\n",
    "num_regs = 2 ** p_bits  # 1024 registers\n",
    "num_identifiers = num_regs * (max_zeros + 1)  # 1024 √ó 33 = 33,792\n",
    "\n",
    "print(\"Option 1: (reg, zeros) identifiers\")\n",
    "print(f\"  P_BITS: {p_bits}\")\n",
    "print(f\"  Registers: {num_regs:,}\")\n",
    "print(f\"  Max zeros: {max_zeros}\")\n",
    "print(f\"  Total identifiers: {num_identifiers:,}\")\n",
    "print(f\"  Matrix size: {num_identifiers:,} √ó {num_identifiers:,}\")\n",
    "\n",
    "print(\"\\nOption 2: Actual tokens (concise languages)\")\n",
    "print(f\"  Chinese: ~80,000 characters\")\n",
    "print(f\"  Matrix size: 80,000 √ó 80,000\")\n",
    "\n",
    "print(f\"\\nVs. millions of unique tokens in full vocabulary\")\n",
    "print(f\"‚úì Single HLLSet combines all n-tokens (perfect fingerprint)\")\n",
    "print(f\"‚úì AM splits it by identifiers for compact storage\")\n",
    "\n",
    "# Memory calculation (sparse matrix)\n",
    "entries_per_mb = 1024 * 1024 / 8  # 8 bytes per entry\n",
    "print(f\"\\nWith sparse storage, only non-zero transitions stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3de50",
   "metadata": {},
   "source": [
    "## 9. AM as Shared Resource - Timing is Critical\n",
    "\n",
    "**Key Architectural Insight**: AM is a shared resource that gets committed/merged after ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6cac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING DEMONSTRATION:\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  During Ingestion (extract_hllset_from_am=True):\n",
      "   ‚îå‚îÄ Build AM from tokens\n",
      "   ‚îú‚îÄ Extract HLLSet from AM (BEFORE commit)\n",
      "   ‚îî‚îÄ Commit AM ‚Üí merges into shared resource\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "  ‚úì LUT committed: n=1, hash=c2b76b7f6c7389f3..., id=2\n",
      "  ‚úì LUT committed: n=2, hash=1915bbc8b2e9217f..., id=1\n",
      "  ‚úì LUT committed: n=3, hash=1ceaf73df40e531d..., id=0\n",
      "\n",
      "   Result: complete_hllset = True\n",
      "   ‚úì HLLSet extracted in time!\n",
      "\n",
      "2Ô∏è‚É£  Skip Extraction (extract_hllset_from_am=False):\n",
      "   ‚îå‚îÄ Build AM from tokens\n",
      "   ‚îú‚îÄ SKIP HLLSet extraction\n",
      "   ‚îî‚îÄ Commit AM ‚Üí merges into shared resource\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "‚úì Extension registered: storage v1.4.4\n",
      "  ‚úì LUT committed: n=1, hash=c2b76b7f6c7389f3..., id=2\n",
      "  ‚úì LUT committed: n=2, hash=1915bbc8b2e9217f..., id=1\n",
      "  ‚úì LUT committed: n=3, hash=1ceaf73df40e531d..., id=0\n",
      "\n",
      "   Result: complete_hllset = False\n",
      "   ‚ö†Ô∏è  No HLLSet (extraction skipped)\n",
      "\n",
      "======================================================================\n",
      "üí° Key Lesson: Extract HLLSet DURING ingestion, BEFORE commit!\n",
      "   After commit, AM is shared - can't derive individual HLLSets\n"
     ]
    }
   ],
   "source": [
    "from core.manifold_os import TokenizationConfig, IngestDriver\n",
    "\n",
    "print(\"TIMING DEMONSTRATION:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: WITH HLLSet extraction (default)\n",
    "print(\"\\n1Ô∏è‚É£  During Ingestion (extract_hllset_from_am=True):\")\n",
    "print(\"   ‚îå‚îÄ Build AM from tokens\")\n",
    "print(\"   ‚îú‚îÄ Extract HLLSet from AM (BEFORE commit)\")\n",
    "print(\"   ‚îî‚îÄ Commit AM ‚Üí merges into shared resource\")\n",
    "\n",
    "os1 = ManifoldOS()\n",
    "result1 = os1.ingest(\"hello world\")\n",
    "print(f\"\\n   Result: complete_hllset = {result1.complete_hllset is not None}\")\n",
    "print(f\"   ‚úì HLLSet extracted in time!\")\n",
    "\n",
    "# Test 2: WITHOUT HLLSet extraction\n",
    "print(\"\\n2Ô∏è‚É£  Skip Extraction (extract_hllset_from_am=False):\")\n",
    "print(\"   ‚îå‚îÄ Build AM from tokens\")\n",
    "print(\"   ‚îú‚îÄ SKIP HLLSet extraction\")\n",
    "print(\"   ‚îî‚îÄ Commit AM ‚Üí merges into shared resource\")\n",
    "\n",
    "config = TokenizationConfig(extract_hllset_from_am=False)\n",
    "os2 = ManifoldOS()\n",
    "driver = IngestDriver(\"no_extract\", config=config)\n",
    "os2.register_driver(driver)\n",
    "driver.wake()\n",
    "\n",
    "result2 = os2.ingest(\"hello world\", driver_id=\"no_extract\")\n",
    "print(f\"\\n   Result: complete_hllset = {result2.complete_hllset is not None}\")\n",
    "print(f\"   ‚ö†Ô∏è  No HLLSet (extraction skipped)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° Key Lesson: Extract HLLSet DURING ingestion, BEFORE commit!\")\n",
    "print(\"   After commit, AM is shared - can't derive individual HLLSets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94286b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Adjacency Matrix Benefits**:\n",
    "\n",
    "1. **Order Preservation**: Captures token transitions\n",
    "2. **Compact Size**: ~100K √ó 100K vs millions of tokens\n",
    "3. **Query Support**: Traverse to reconstruct order\n",
    "4. **Frequency Tracking**: Higher frequencies = more common transitions\n",
    "5. **START/END**: Clear sequence boundaries\n",
    "\n",
    "**Workflow**:\n",
    "- **Ingestion**: Build AM with transitions\n",
    "- **Query**: Traverse AM to reconstruct order\n",
    "- **LUT**: Map identifiers back to original tokens\n",
    "\n",
    "---\n",
    "\n",
    "## HLLSet Paradigm Shift: Freedom of Representation\n",
    "\n",
    "**Traditional Data Structures** operate with a \"copy and paste\" mindset:\n",
    "- Data must be stored in specific formats\n",
    "- Transformations create separate copies\n",
    "- Identity means exact structural match\n",
    "\n",
    "**HLLSet Algebra** offers transformational freedom:\n",
    "- **Same dataset** = any equal transformation\n",
    "- Single HLLSet combining all n-tokens ‚â° Separate HLLSets per group\n",
    "- (reg, zeros) identifiers ‚â° actual tokens (for concise languages)\n",
    "- **Freedom**: Choose the representation that best serves your use case\n",
    "\n",
    "**Examples of Equivalent Representations**:\n",
    "```python\n",
    "# These are all equivalent \"views\" of the same data:\n",
    "hll_combined = HLLSet.from_batch(tokens_1 + tokens_2 + tokens_3)\n",
    "hll_separate = hll_1.union(hll_2).union(hll_3)\n",
    "# Both have identical cardinality and support same operations\n",
    "\n",
    "# AM can split by:\n",
    "am_identifiers = build_am(hll, key=lambda t: (reg(t), zeros(t)))  # Compact\n",
    "am_tokens = build_am(hll, key=lambda t: t)                        # Explicit\n",
    "# Choice depends on matrix size vs token vocabulary\n",
    "```\n",
    "\n",
    "**Learning HLLSet Algebra**:\n",
    "- Operations: union, intersection, difference, similarity\n",
    "- Transformations preserve cardinality relationships\n",
    "- No forced \"canonical\" representation\n",
    "- Algebra guides you to optimal representations\n",
    "\n",
    "This flexibility is **powerful**: choose representations that optimize for your specific needs (memory, speed, interpretability) while maintaining mathematical equivalence.\n",
    "\n",
    "**Next**: See [04_kernel_entanglement.ipynb](04_kernel_entanglement.ipynb) for kernel operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
