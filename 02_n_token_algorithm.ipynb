{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459e70eb",
   "metadata": {},
   "source": [
    "# N-Token Ingestion Algorithm\n",
    "\n",
    "This notebook demonstrates the n-token ingestion algorithm with a single HLLSet combining all n-token groups.\n",
    "\n",
    "**Key Architecture**:\n",
    "- Generate n-token groups (1-tokens, 2-tokens, 3-tokens) via sliding window\n",
    "- **Single HLLSet** combining all n-tokens together (perfect fingerprint)\n",
    "- In AM matrix: HLLSet split by (reg, zeros) identifiers for disambiguation\n",
    "- Build Lookup Tables (LUTs) to map identifiers → original tokens\n",
    "- Preserve implicit order through sequential n-token generation\n",
    "\n",
    "**Trade-off**: Cardinality tripled (all n-tokens in one set), but acceptable for BSS metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffe222",
   "metadata": {},
   "source": [
    "## 1. Basic N-Token Generation\n",
    "\n",
    "Understanding how n-tokens are generated from a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79484026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps']\n",
      "\n",
      "Number of n-token groups: 3\n",
      "\n",
      "1-tokens: 5 groups\n",
      "  [('the',), ('quick',), ('brown',), ('fox',), ('jumps',)]...\n",
      "\n",
      "2-tokens: 4 groups\n",
      "  [('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps')]...\n",
      "\n",
      "3-tokens: 3 groups\n",
      "  [('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps')]...\n"
     ]
    }
   ],
   "source": [
    "from core.manifold_os import ManifoldOS\n",
    "\n",
    "# Create ManifoldOS instance\n",
    "os = ManifoldOS()\n",
    "\n",
    "# Ingest a simple sentence\n",
    "text = \"the quick brown fox jumps\"\n",
    "result = os.ingest(text)\n",
    "\n",
    "print(\"Original tokens:\", result.original_tokens)\n",
    "print(f\"\\nNumber of n-token groups: {len(result.n_token_groups)}\")\n",
    "\n",
    "# Show each n-token group\n",
    "for n, tokens in sorted(result.n_token_groups.items()):\n",
    "    print(f\"\\n{n}-tokens: {len(tokens)} groups\")\n",
    "    print(f\"  {tokens[:5]}...\")  # Show first 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c81706",
   "metadata": {},
   "source": [
    "## 2. Single HLLSet with Multiple N-Token Groups\n",
    "\n",
    "Ingestion creates a **single HLLSet** combining all n-tokens together (1-tokens + 2-tokens + 3-tokens).\n",
    "\n",
    "**Architecture**:\n",
    "- Single HLLSet = perfect fingerprint for the document\n",
    "- Full support for set operations (union, intersection, similarity)\n",
    "- In AM matrix, this HLLSet is split by (reg, zeros) identifiers for disambiguation\n",
    "- For concise languages (e.g., Chinese ~80K characters), can split by actual tokens\n",
    "\n",
    "**Trade-off**:\n",
    "- Cardinality gets tripled (all n-tokens in one set)\n",
    "- This is acceptable because:\n",
    "  - Cardinality rarely used directly\n",
    "  - In BSS metric, it's just a scale coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad89377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HLLSets: 3\n",
      "Note: These are separate per n-group for convenience,\n",
      "but conceptually form a single combined HLLSet\n",
      "\n",
      "1-token HLLSet:\n",
      "  Cardinality: 5.01\n",
      "  Name: 61e1ce87...\n",
      "  Backend: C/Cython\n",
      "\n",
      "2-token HLLSet:\n",
      "  Cardinality: 4.01\n",
      "  Name: 9bee32c5...\n",
      "  Backend: C/Cython\n",
      "\n",
      "3-token HLLSet:\n",
      "  Cardinality: 3.00\n",
      "  Name: 83e18592...\n",
      "  Backend: C/Cython\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check HLLSets created\n",
    "print(f\"Number of HLLSets: {len(result.hllsets)}\")\n",
    "print(\"Note: These are separate per n-group for convenience,\")\n",
    "print(\"but conceptually form a single combined HLLSet\\n\")\n",
    "\n",
    "for n, hllset in sorted(result.hllsets.items()):\n",
    "    card = hllset.cardinality()\n",
    "    print(f\"{n}-token HLLSet:\")\n",
    "    print(f\"  Cardinality: {card:.2f}\")\n",
    "    print(f\"  Name: {hllset.short_name}...\")\n",
    "    print(f\"  Backend: {hllset.backend}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637bb0b0",
   "metadata": {},
   "source": [
    "## 3. Lookup Tables (LUTs)\n",
    "\n",
    "LUTs map (reg, zeros) identifiers back to original token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0a7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LUTs: 3\n",
      "\n",
      "1-token LUT:\n",
      "  Entries: 5\n",
      "\n",
      "  (989, 0):\n",
      "    Tokens: [('the',)]\n",
      "    Hashes: [14035282076209885149]\n",
      "\n",
      "  (326, 1):\n",
      "    Tokens: [('quick',)]\n",
      "    Hashes: [12712478176260299078]\n",
      "\n",
      "  (1018, 0):\n",
      "    Tokens: [('brown',)]\n",
      "    Hashes: [18241176695549286394]\n",
      "\n",
      "2-token LUT:\n",
      "  Entries: 4\n",
      "\n",
      "  (408, 0):\n",
      "    Tokens: [('the', 'quick')]\n",
      "    Hashes: [13181924135014296984]\n",
      "\n",
      "  (378, 3):\n",
      "    Tokens: [('quick', 'brown')]\n",
      "    Hashes: [589506280862491002]\n",
      "\n",
      "  (555, 0):\n",
      "    Tokens: [('brown', 'fox')]\n",
      "    Hashes: [4326298031740651051]\n",
      "\n",
      "3-token LUT:\n",
      "  Entries: 3\n",
      "\n",
      "  (174, 2):\n",
      "    Tokens: [('the', 'quick', 'brown')]\n",
      "    Hashes: [15360034458666004654]\n",
      "\n",
      "  (740, 1):\n",
      "    Tokens: [('quick', 'brown', 'fox')]\n",
      "    Hashes: [3896194499370466020]\n",
      "\n",
      "  (177, 2):\n",
      "    Tokens: [('brown', 'fox', 'jumps')]\n",
      "    Hashes: [5489326747953926321]\n"
     ]
    }
   ],
   "source": [
    "# Examine LUTs\n",
    "print(f\"Number of LUTs: {len(result.luts)}\")\n",
    "\n",
    "for n, lut in sorted(result.luts.items()):\n",
    "    print(f\"\\n{n}-token LUT:\")\n",
    "    print(f\"  Entries: {len(lut)}\")\n",
    "    \n",
    "    # Show a few entries\n",
    "    for (reg, zeros), record in list(lut.items())[:3]:\n",
    "        print(f\"\\n  ({reg}, {zeros}):\")\n",
    "        print(f\"    Tokens: {record.tokens}\")\n",
    "        print(f\"    Hashes: {record.hashes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f2a74",
   "metadata": {},
   "source": [
    "## 4. Implicit Order Preservation\n",
    "\n",
    "The order of n-tokens implicitly preserves document structure:\n",
    "1. All 1-tokens come first (in order)\n",
    "2. Then all 2-tokens (in order)\n",
    "3. Then all 3-tokens (in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca05a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit order (12 items):\n",
      "\n",
      "  ('the',)\n",
      "  ('quick',)\n",
      "  ('brown',)\n",
      "  ('fox',)\n",
      "  ('jumps',)\n",
      "  ('the', 'quick')\n",
      "  ('quick', 'brown')\n",
      "  ('brown', 'fox')\n",
      "  ('fox', 'jumps')\n",
      "  ('the', 'quick', 'brown')\n",
      "  ('quick', 'brown', 'fox')\n",
      "  ('brown', 'fox', 'jumps')\n",
      "\n",
      "✓ Order preserved: 1-tokens, then 2-tokens, then 3-tokens\n"
     ]
    }
   ],
   "source": [
    "# Get implicit order\n",
    "implicit_order = result.get_implicit_order()\n",
    "\n",
    "print(f\"Implicit order ({len(implicit_order)} items):\\n\")\n",
    "for item in implicit_order[:15]:  # Show first 15\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n✓ Order preserved: 1-tokens, then 2-tokens, then 3-tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9841a0f",
   "metadata": {},
   "source": [
    "## 5. Disambiguation Example\n",
    "\n",
    "When multiple tokens hash to the same (reg, zeros), LUT disambiguates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58ffec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['hello', 'world', 'from', 'multiple', 'representations', 'and', 'multiple', 'contexts']\n",
      "\n",
      "1-token LUT entries: 7\n",
      "2-token LUT entries: 7\n",
      "3-token LUT entries: 6\n",
      "\n",
      "Disambiguated entries (multiple tokens → same identifier):\n"
     ]
    }
   ],
   "source": [
    "# Ingest text with potential hash collisions\n",
    "text2 = \"hello world from multiple representations and multiple contexts\"\n",
    "result2 = os.ingest(text2)\n",
    "\n",
    "print(\"Original tokens:\", result2.original_tokens)\n",
    "print(f\"\\n1-token LUT entries: {len(result2.luts[1])}\")\n",
    "print(f\"2-token LUT entries: {len(result2.luts[2])}\")\n",
    "print(f\"3-token LUT entries: {len(result2.luts[3])}\")\n",
    "\n",
    "# Find entries with multiple token sequences (disambiguated)\n",
    "print(\"\\nDisambiguated entries (multiple tokens → same identifier):\")\n",
    "for n, lut in result2.luts.items():\n",
    "    for (reg, zeros), record in lut.items():\n",
    "        if len(record.tokens) > 1:\n",
    "            print(f\"\\n{n}-token ({reg}, {zeros}):\")\n",
    "            print(f\"  Tokens: {record.tokens}\")\n",
    "            break  # Show just one example per n-group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f1a6d",
   "metadata": {},
   "source": [
    "## 6. Batch Processing with N-Tokens\n",
    "\n",
    "Process multiple documents, each getting their own n-token representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396fdca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 documents\n",
      "\n",
      "Document 1:\n",
      "  Tokens: 5\n",
      "  HLLSets: 3\n",
      "  1-token cardinality: 5.01\n",
      "\n",
      "Document 2:\n",
      "  Tokens: 5\n",
      "  HLLSets: 3\n",
      "  1-token cardinality: 5.01\n",
      "\n",
      "Document 3:\n",
      "  Tokens: 5\n",
      "  HLLSets: 3\n",
      "  1-token cardinality: 5.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple documents\n",
    "documents = [\n",
    "    \"first document about data science\",\n",
    "    \"second document about machine learning\",\n",
    "    \"third document about artificial intelligence\"\n",
    "]\n",
    "\n",
    "results = os.ingest_batch(documents)\n",
    "\n",
    "print(f\"Processed {len(results)} documents\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Tokens: {len(result.original_tokens)}\")\n",
    "    print(f\"  HLLSets: {len(result.hllsets)}\")\n",
    "    print(f\"  1-token cardinality: {result.hllsets[1].cardinality():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073a589",
   "metadata": {},
   "source": [
    "## 7. Custom Tokenization Configuration\n",
    "\n",
    "Configure n-token groups and tokenization behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74469ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-token groups: [1, 2, 3, 4]\n",
      "HLLSets created: 4\n",
      "\n",
      "4-token groups: 1\n",
      "Examples: [('testing', 'custom', 'tokenization', 'configuration')]\n"
     ]
    }
   ],
   "source": [
    "from core.manifold_os import TokenizationConfig, IngestDriver\n",
    "\n",
    "# Custom config with different n-token groups\n",
    "config = TokenizationConfig(\n",
    "    use_n_tokens=True,\n",
    "    n_token_groups=[1, 2, 3, 4],  # Include 4-tokens\n",
    "    maintain_order=True,\n",
    "    lowercase=True,\n",
    "    min_token_length=2\n",
    ")\n",
    "\n",
    "# Create and register new driver with custom config\n",
    "custom_driver = IngestDriver(\"ingest_custom\", config=config)\n",
    "os.register_driver(custom_driver)\n",
    "custom_driver.wake()  # Activate driver\n",
    "\n",
    "# Ingest using custom driver\n",
    "result_custom = os.ingest(\"testing custom tokenization configuration\", driver_id=\"ingest_custom\")\n",
    "\n",
    "print(f\"N-token groups: {list(result_custom.n_token_groups.keys())}\")\n",
    "print(f\"HLLSets created: {len(result_custom.hllsets)}\")\n",
    "print(f\"\\n4-token groups: {len(result_custom.n_token_groups[4])}\")\n",
    "print(f\"Examples: {result_custom.n_token_groups[4][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac7476",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Benefits of N-Token Algorithm**:\n",
    "\n",
    "1. **Single HLLSet Fingerprint**: All n-tokens combined into one HLLSet\n",
    "   - Perfect fingerprint for document\n",
    "   - Full support for set operations (union, intersection, similarity)\n",
    "   \n",
    "2. **Multiple Representations**: Each n-group provides different granularity\n",
    "   - 1-tokens, 2-tokens, 3-tokens, etc.\n",
    "   - Better context and disambiguation\n",
    "   \n",
    "3. **AM Matrix Splitting**: HLLSet split by (reg, zeros) identifiers in AM\n",
    "   - Compact representation (~100K identifiers vs millions of tokens)\n",
    "   - For concise languages: can split by actual tokens (e.g., Chinese ~80K)\n",
    "   \n",
    "4. **Disambiguation via LUTs**: Map identifiers back to original tokens\n",
    "   - Resolve hash collisions\n",
    "   - Preserve original token sequences\n",
    "   \n",
    "5. **Order Preservation**: Implicit order through sequential generation\n",
    "   - 1-tokens first, then 2-tokens, then 3-tokens\n",
    "   \n",
    "6. **Cardinality Trade-off**: Tripled cardinality is acceptable\n",
    "   - Rarely used directly\n",
    "   - BSS metric treats it as scale coefficient\n",
    "\n",
    "**Next**: See `03_adjacency_matrix.ipynb` for order reconstruction with Adjacency Matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
