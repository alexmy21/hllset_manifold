{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e1d0f7",
   "metadata": {},
   "source": [
    "# Database Ingestion Workbook\n",
    "\n",
    "**Real-world workflow**: Ingest CSV data into HLLSet-based semantic database\n",
    "\n",
    "## Overview\n",
    "\n",
    "This workbook demonstrates columnar database ingestion:\n",
    "1. Convert CSV files → DuckDB (one-time)\n",
    "2. Ingest database column-by-column into HLLSets\n",
    "3. Create data + metadata hierarchies with explicit entanglement\n",
    "4. Query and analyze ingested data semantically\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "CSV Files → DuckDB → Columnar Ingestion → HLLSet Hierarchy\n",
    "                      ├─ Data: DB → Tables → Columns\n",
    "                      ├─ Metadata: Raw + Fingerprints + SHA1 IDs\n",
    "                      └─ Entanglement: Data ↔ Metadata\n",
    "```\n",
    "\n",
    "**Key Benefit**: Thousands of columns vs millions/billions of rows = massive compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdae1a",
   "metadata": {},
   "source": [
    "## Step 1: Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad76508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ ManifoldOS initialized\n",
      "  Storage: .manifold_storage\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from pathlib import Path\n",
    "import json\n",
    "from core.manifold_os import ManifoldOS\n",
    "from core.db_ingestion import DatabaseIngestionSystem\n",
    "from tools.db_query_helper import DatabaseQueryHelper\n",
    "\n",
    "# Initialize ManifoldOS with unified storage\n",
    "manifold = ManifoldOS()\n",
    "print(f\"✓ ManifoldOS initialized\")\n",
    "print(f\"  Storage: {manifold.store.storage_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f5180",
   "metadata": {},
   "source": [
    "## Step 2: CSV to DuckDB Conversion\n",
    "\n",
    "**One-time operation**: Convert directory of CSV files into single DuckDB database.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Update `csv_directory` path below to point to your CSV files\n",
    "2. Run the conversion cell (uncomment first)\n",
    "3. Verify the database was created\n",
    "\n",
    "**Note**: This creates a persistent DuckDB file. Only run once, or when you want to refresh the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1d8779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV directory: /home/alexmy/Downloads/POC/demo/pub/finance/\n",
      "Database path: data/finance_data.duckdb\n",
      "\n",
      "To convert CSVs to DuckDB, uncomment and run:\n",
      "  !python tools/csv2db.py /home/alexmy/Downloads/POC/demo/pub/finance/ data/finance_data.duckdb\n"
     ]
    }
   ],
   "source": [
    "# Configure paths\n",
    "csv_directory = \"/home/alexmy/Downloads/POC/demo/pub/finance/\"  # UPDATE THIS!\n",
    "db_path = Path(\"./data/finance_data.duckdb\")  # Use different name to avoid conflicts\n",
    "\n",
    "print(f\"CSV directory: {csv_directory}\")\n",
    "print(f\"Database path: {db_path}\")\n",
    "print()\n",
    "print(\"To convert CSVs to DuckDB, uncomment and run:\")\n",
    "print(f\"  !python tools/csv2db.py {csv_directory} {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f29f568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files\n",
      "Creating database: data/finance_data.duckdb\n",
      "Ingesting: fdic_failed_banklist.csv → fdic_failed_banklist... ✓ (540 rows)\n",
      "Ingesting: global_sports_finances.csv → global_sports_finances... ✗ Failed: Invalid Input Error: CSV Error on Line: 60\n",
      "Original Line: Pau Gasol,$21.8 M,$19.3 M,$2.5 M,Basketball\n",
      "Invalid unicode (byte sequence mismatch) detected. This file is not utf-8 encoded.\n",
      "\n",
      "Possible Solution: Set the correct encoding, if available, to read this CSV File (e.g., encoding='UTF-16')\n",
      "Possible Solution: Enable ignore errors (ignore_errors=true) to skip this row\n",
      "\n",
      "  file = /home/alexmy/Downloads/POC/demo/pub/finance/global_sports_finances.csv\n",
      "  delimiter = , (Auto-Detected)\n",
      "  quote = \" (Auto-Detected)\n",
      "  escape = (empty) (Auto-Detected)\n",
      "  new_line = Single-Line File (Auto-Detected)\n",
      "  header = true (Set By User)\n",
      "  skip_rows = 0 (Auto-Detected)\n",
      "  comment = (empty) (Auto-Detected)\n",
      "  strict_mode = true (Auto-Detected)\n",
      "  date_format =  (Auto-Detected)\n",
      "  timestamp_format =  (Auto-Detected)\n",
      "  null_padding = 0\n",
      "  sample_size = 18446744073709549568\n",
      "  ignore_errors = false\n",
      "  all_varchar = 0\n",
      "\n",
      "\n",
      "\n",
      "LINE 3:                 SELECT * FROM read_csv_auto('/home/alexmy/Downloads/POC/demo/pub/finance...\n",
      "                                      ^\n",
      "Ingesting: inc5000_companies.csv → inc5000_companies... ✗ Failed: Invalid Input Error: CSV Error on Line: 1012\n",
      "Original Line: 1010,Inc5000 Company List,Inc5000 Company List,1010,http://www.inc.com/inc5000list/json/inc5000_2014.json,8109,1010,210,Unitrends,unitrends,South Carolina,SC,Columbia,Columbia SC,441.1668,54398909,IT Services,4\n",
      "Invalid unicode (byte sequence mismatch) detected. This file is not utf-8 encoded.\n",
      "\n",
      "Possible Solution: Set the correct encoding, if available, to read this CSV File (e.g., encoding='UTF-16')\n",
      "Possible Solution: Enable ignore errors (ignore_errors=true) to skip this row\n",
      "\n",
      "  file = /home/alexmy/Downloads/POC/demo/pub/finance/inc5000_companies.csv\n",
      "  delimiter = , (Auto-Detected)\n",
      "  quote = \" (Auto-Detected)\n",
      "  escape = (empty) (Auto-Detected)\n",
      "  new_line = Single-Line File (Auto-Detected)\n",
      "  header = true (Set By User)\n",
      "  skip_rows = 0 (Auto-Detected)\n",
      "  comment = (empty) (Auto-Detected)\n",
      "  strict_mode = true (Auto-Detected)\n",
      "  date_format =  (Auto-Detected)\n",
      "  timestamp_format =  (Auto-Detected)\n",
      "  null_padding = 0\n",
      "  sample_size = 18446744073709549568\n",
      "  ignore_errors = false\n",
      "  all_varchar = 0\n",
      "\n",
      "\n",
      "\n",
      "LINE 3:                 SELECT * FROM read_csv_auto('/home/alexmy/Downloads/POC/demo/pub/finance...\n",
      "                                      ^\n",
      "Ingesting: investment_company_series_class.csv → investment_company_series_class... ✓ (41283 rows)\n",
      "Ingesting: nfl_offensive_player_stats_1999_2013.csv → nfl_offensive_player_stats_1999_2013... ✓ (4625 rows)\n",
      "Ingesting: public_company_bankruptcy_cases.csv → public_company_bankruptcy_cases... ✓ (129 rows)\n",
      "Ingesting: subprime_2006_distributed.csv → subprime_2006_distributed... ✓ (210 rows)\n",
      "Ingesting: top_teams_payroll.csv → top_teams_payroll... ✓ (294 rows)\n",
      "\n",
      "======================================================================\n",
      "Successfully created 6 tables:\n",
      "  • fdic_failed_banklist           (fdic_failed_banklist.csv      )        540 rows\n",
      "  • investment_company_series_class (investment_company_series_class.csv)     41,283 rows\n",
      "  • nfl_offensive_player_stats_1999_2013 (nfl_offensive_player_stats_1999_2013.csv)      4,625 rows\n",
      "  • public_company_bankruptcy_cases (public_company_bankruptcy_cases.csv)        129 rows\n",
      "  • subprime_2006_distributed      (subprime_2006_distributed.csv )        210 rows\n",
      "  • top_teams_payroll              (top_teams_payroll.csv         )        294 rows\n",
      "\n",
      "Failed to create 2 tables:\n",
      "  • global_sports_finances.csv: Invalid Input Error: CSV Error on Line: 60\n",
      "Original Line: Pau Gasol,$21.8 M,$19.3 M,$2.5 M,Basketball\n",
      "Invalid unicode (byte sequence mismatch) detected. This file is not utf-8 encoded.\n",
      "\n",
      "Possible Solution: Set the correct encoding, if available, to read this CSV File (e.g., encoding='UTF-16')\n",
      "Possible Solution: Enable ignore errors (ignore_errors=true) to skip this row\n",
      "\n",
      "  file = /home/alexmy/Downloads/POC/demo/pub/finance/global_sports_finances.csv\n",
      "  delimiter = , (Auto-Detected)\n",
      "  quote = \" (Auto-Detected)\n",
      "  escape = (empty) (Auto-Detected)\n",
      "  new_line = Single-Line File (Auto-Detected)\n",
      "  header = true (Set By User)\n",
      "  skip_rows = 0 (Auto-Detected)\n",
      "  comment = (empty) (Auto-Detected)\n",
      "  strict_mode = true (Auto-Detected)\n",
      "  date_format =  (Auto-Detected)\n",
      "  timestamp_format =  (Auto-Detected)\n",
      "  null_padding = 0\n",
      "  sample_size = 18446744073709549568\n",
      "  ignore_errors = false\n",
      "  all_varchar = 0\n",
      "\n",
      "\n",
      "\n",
      "LINE 3:                 SELECT * FROM read_csv_auto('/home/alexmy/Downloads/POC/demo/pub/finance...\n",
      "                                      ^\n",
      "  • inc5000_companies.csv: Invalid Input Error: CSV Error on Line: 1012\n",
      "Original Line: 1010,Inc5000 Company List,Inc5000 Company List,1010,http://www.inc.com/inc5000list/json/inc5000_2014.json,8109,1010,210,Unitrends,unitrends,South Carolina,SC,Columbia,Columbia SC,441.1668,54398909,IT Services,4\n",
      "Invalid unicode (byte sequence mismatch) detected. This file is not utf-8 encoded.\n",
      "\n",
      "Possible Solution: Set the correct encoding, if available, to read this CSV File (e.g., encoding='UTF-16')\n",
      "Possible Solution: Enable ignore errors (ignore_errors=true) to skip this row\n",
      "\n",
      "  file = /home/alexmy/Downloads/POC/demo/pub/finance/inc5000_companies.csv\n",
      "  delimiter = , (Auto-Detected)\n",
      "  quote = \" (Auto-Detected)\n",
      "  escape = (empty) (Auto-Detected)\n",
      "  new_line = Single-Line File (Auto-Detected)\n",
      "  header = true (Set By User)\n",
      "  skip_rows = 0 (Auto-Detected)\n",
      "  comment = (empty) (Auto-Detected)\n",
      "  strict_mode = true (Auto-Detected)\n",
      "  date_format =  (Auto-Detected)\n",
      "  timestamp_format =  (Auto-Detected)\n",
      "  null_padding = 0\n",
      "  sample_size = 18446744073709549568\n",
      "  ignore_errors = false\n",
      "  all_varchar = 0\n",
      "\n",
      "\n",
      "\n",
      "LINE 3:                 SELECT * FROM read_csv_auto('/home/alexmy/Downloads/POC/demo/pub/finance...\n",
      "                                      ^\n",
      "\n",
      "======================================================================\n",
      "Database Statistics:\n",
      "  Total tables: 6\n",
      "  Estimated size: 47,081 bytes (0.04 MB)\n",
      "\n",
      "Top 10 tables by column count:\n",
      "  • nfl_offensive_player_stats_1999_2013  26 columns\n",
      "  • investment_company_series_class  20 columns\n",
      "  • duckdb_columns                  18 columns\n",
      "  • duckdb_tables                   16 columns\n",
      "  • duckdb_constraints              15 columns\n",
      "  • duckdb_indexes                  14 columns\n",
      "  • duckdb_types                    13 columns\n",
      "  • duckdb_views                    12 columns\n",
      "  • duckdb_logs                     10 columns\n",
      "  • duckdb_databases                10 columns\n",
      "\n",
      "Database saved to: data/finance_data.duckdb\n",
      "======================================================================\n",
      "✓ Database found: data/finance_data.duckdb\n",
      "  Size: 4.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below and run to perform conversion:\n",
    "!python tools/csv2db.py {csv_directory} {db_path}\n",
    "\n",
    "# Verify database exists\n",
    "if db_path.exists():\n",
    "    size_mb = db_path.stat().st_size / 1024 / 1024\n",
    "    print(f\"✓ Database found: {db_path}\")\n",
    "    print(f\"  Size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ Database not found: {db_path}\")\n",
    "    print(\"  Run the conversion command above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24a53c",
   "metadata": {},
   "source": [
    "### Explore Database (Optional)\n",
    "\n",
    "Quick peek at what's in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089ef247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database contains 6 tables:\n",
      "\n",
      "  • fdic_failed_banklist                     ( 7 columns)\n",
      "  • investment_company_series_class          (20 columns)\n",
      "  • nfl_offensive_player_stats_1999_2013     (26 columns)\n",
      "  • public_company_bankruptcy_cases          ( 5 columns)\n",
      "  • subprime_2006_distributed                ( 5 columns)\n",
      "  • top_teams_payroll                        ( 9 columns)\n"
     ]
    }
   ],
   "source": [
    "# Explore database contents\n",
    "if db_path.exists():\n",
    "    import duckdb\n",
    "    \n",
    "    try:\n",
    "        conn = duckdb.connect(str(db_path), read_only=True)\n",
    "        \n",
    "        # List tables\n",
    "        tables = conn.execute(\"\"\"\n",
    "            SELECT table_name, \n",
    "                   (SELECT COUNT(*) FROM duckdb_columns() c \n",
    "                    WHERE c.table_name = t.table_name) as column_count\n",
    "            FROM duckdb_tables() t\n",
    "            WHERE schema_name = 'main'\n",
    "            ORDER BY table_name\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        print(f\"Database contains {len(tables)} tables:\\n\")\n",
    "        for table_name, col_count in tables[:20]:  # Show first 20\n",
    "            print(f\"  • {table_name:40s} ({col_count:2d} columns)\")\n",
    "        \n",
    "        if len(tables) > 20:\n",
    "            print(f\"  ... and {len(tables) - 20} more tables\")\n",
    "        \n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not explore database: {e}\")\n",
    "        print(\"\\nThis is usually because the database is already open.\")\n",
    "        print(\"Solution: Use a different database filename (see cell above)\")\n",
    "        print(\"or skip this exploration cell and continue with ingestion.\")\n",
    "else:\n",
    "    print(\"Database not found - skip this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f857ab2",
   "metadata": {},
   "source": [
    "## Step 3: Columnar Ingestion\n",
    "\n",
    "Now ingest the database column by column into HLLSets.\n",
    "\n",
    "### What happens:\n",
    "- Each column → HLLSet (all distinct values)\n",
    "- Each table → HLLSet (union of columns)\n",
    "- Database → HLLSet (union of tables)\n",
    "- Metadata hierarchy (raw + fingerprints + SHA1 IDs)\n",
    "- Explicit entanglements (data ↔ metadata)\n",
    "\n",
    "**Time estimate**: ~5-15 minutes for 200 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b51aec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ingestion system initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize ingestion system\n",
    "db_ingestion = DatabaseIngestionSystem(manifold)\n",
    "print(\"✓ Ingestion system initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e161f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting columnar ingestion...\n",
      "\n",
      "======================================================================\n",
      "COLUMNAR DATABASE INGESTION\n",
      "======================================================================\n",
      "Connected to database: data/finance_data.duckdb\n",
      "\n",
      "Database: main\n",
      "Tables: 6\n",
      "\n",
      "Ingesting table: fdic_failed_banklist\n",
      "  Columns: 7, Rows: 540\n",
      "    Processing column: Bank_Name... ✓ (515 distinct values)\n",
      "    Processing column: City... ✓ (411 distinct values)\n",
      "    Processing column: ST... ✓ (46 distinct values)\n",
      "    Processing column: CERT... ✓ (545 distinct values)\n",
      "    Processing column: Acquiring_Institution... ✓ (286 distinct values)\n",
      "    Processing column: Closing_Date... ✓ (233 distinct values)\n",
      "    Processing column: Updated_Date... ✓ (108 distinct values)\n",
      "\n",
      "Ingesting table: investment_company_series_class\n",
      "  Columns: 20, Rows: 41,283\n",
      "    Processing column: Reporting File Number... ✓ (2469 distinct values)\n",
      "    Processing column: CIK Number... ✓ (2556 distinct values)\n",
      "    Processing column: Name of Investment Company... ✓ (2372 distinct values)\n",
      "    Processing column: Organization Type... ✓ (8 distinct values)\n",
      "    Processing column: Series ID... ✓ (13498 distinct values)\n",
      "    Processing column: Series Name... ✓ (13215 distinct values)\n",
      "    Processing column: Class ID... ✓ (42034 distinct values)\n",
      "    Processing column: Class Name... ✓ (11900 distinct values)\n",
      "    Processing column: Class Ticker... ✓ (28523 distinct values)\n",
      "    Processing column: Address_1... ✓ (840 distinct values)\n",
      "    Processing column: Address_2... ✓ (407 distinct values)\n",
      "    Processing column: City... ✓ (247 distinct values)\n",
      "    Processing column: State... ✓ (53 distinct values)\n",
      "    Processing column: Zip Code... ✓ (489 distinct values)\n",
      "    Processing column: column14... ✓ (2 distinct values)\n",
      "    Processing column: column15... ✓ (7 distinct values)\n",
      "    Processing column: column16... ✓ (2 distinct values)\n",
      "    Processing column: column17... ✓ (2 distinct values)\n",
      "    Processing column: column18... ✓ (2 distinct values)\n",
      "    Processing column: column19... ✓ (2 distinct values)\n",
      "\n",
      "Ingesting table: nfl_offensive_player_stats_1999_2013\n",
      "  Columns: 26, Rows: 4,625\n",
      "    Processing column: id... ✓ (4542 distinct values)\n",
      "    Processing column: Year... ✓ (18 distinct values)\n",
      "    Processing column: Name... ✓ (4623 distinct values)\n",
      "    Processing column: Position... ✓ (21 distinct values)\n",
      "    Processing column: HeightFeet... ✓ (3 distinct values)\n",
      "    Processing column: HeightInches... ✓ (41 distinct values)\n",
      "    Processing column: Weight... ✓ (206 distinct values)\n",
      "    Processing column: Arms... ✓ (71 distinct values)\n",
      "    Processing column: Hands... ✓ (34 distinct values)\n",
      "    Processing column: FortyYD... ✓ (158 distinct values)\n",
      "    Processing column: TwentyYD... ✓ (36 distinct values)\n",
      "    Processing column: TenYD... ✓ (52 distinct values)\n",
      "    Processing column: TwentySS... ✓ (152 distinct values)\n",
      "    Processing column: ThreeCone... ✓ (165 distinct values)\n",
      "    Processing column: Vertical... ✓ (54 distinct values)\n",
      "    Processing column: Broad... ✓ (63 distinct values)\n",
      "    Processing column: Bench... ✓ (53 distinct values)\n",
      "    Processing column: Round... ✓ (9 distinct values)\n",
      "    Processing column: College... ✓ (271 distinct values)\n",
      "    Processing column: Pick... ✓ (1336 distinct values)\n",
      "    Processing column: PickRound... ✓ (60 distinct values)\n",
      "    Processing column: PickTotal... ✓ (265 distinct values)\n",
      "    Processing column: FirstName... ✓ (1519 distinct values)\n",
      "    Processing column: LastName... ✓ (2565 distinct values)\n",
      "    Processing column: HeightInchesTotal... ✓ (45 distinct values)\n",
      "    Processing column: Wonderlic... ✓ (42 distinct values)\n",
      "\n",
      "Ingesting table: public_company_bankruptcy_cases\n",
      "  Columns: 5, Rows: 129\n",
      "    Processing column: DISTRICT... ✓ (9 distinct values)\n",
      "    Processing column: STATE... ✓ (26 distinct values)\n",
      "    Processing column: COMPANY_NAME... ✓ (127 distinct values)\n",
      "    Processing column: ASSETS_MILLIONS... ✓ (125 distinct values)\n",
      "    Processing column: LIABILITIES_MILLIONS... ✓ (123 distinct values)\n",
      "\n",
      "Ingesting table: subprime_2006_distributed\n",
      "  Columns: 5, Rows: 210\n",
      "    Processing column: IDD... ✓ (211 distinct values)\n",
      "    Processing column: CODE... ✓ (7 distinct values)\n",
      "    Processing column: ID... ✓ (208 distinct values)\n",
      "    Processing column: MH... ✓ (2 distinct values)\n",
      "    Processing column: NAME... ✓ (200 distinct values)\n",
      "\n",
      "Ingesting table: top_teams_payroll\n",
      "  Columns: 9, Rows: 294\n",
      "    Processing column: rank... ✓ (297 distinct values)\n",
      "    Processing column: last_year_rank... ✓ (270 distinct values)\n",
      "    Processing column: team_league... ✓ (298 distinct values)\n",
      "    Processing column: avg_annual_pay_per_player... ✓ (303 distinct values)\n",
      "    Processing column: pct_chg_from_last_year... ✓ (202 distinct values)\n",
      "    Processing column: total_payroll... ✓ (282 distinct values)\n",
      "    Processing column: rank_of_total_payroll... ✓ (297 distinct values)\n",
      "    Processing column: avg_player_5_yr_earnings... ✓ (193 distinct values)\n",
      "    Processing column: pct_chg_last_five_yrs... ✓ (119 distinct values)\n",
      "\n",
      "======================================================================\n",
      "INGESTION COMPLETE\n",
      "======================================================================\n",
      "Database HLLSet cardinality: 133,580\n",
      "Tables ingested: 6\n",
      "Columns ingested: 72\n",
      "\n",
      "✓ Ingestion complete!\n",
      "  Result saved to: data/ingestion_result.json\n",
      "  Database HLLSet ID: c06c7c5534f18180b450cef0643c60cf2eaee256\n",
      "  Tables ingested: 6\n"
     ]
    }
   ],
   "source": [
    "# Perform columnar ingestion\n",
    "if db_path.exists():\n",
    "    print(\"Starting columnar ingestion...\\n\")\n",
    "    ingestion_result = db_ingestion.ingest_database(db_path)\n",
    "    \n",
    "    # Save result for later use\n",
    "    result_path = Path('./data/ingestion_result.json')\n",
    "    with open(result_path, 'w') as f:\n",
    "        json.dump(ingestion_result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Ingestion complete!\")\n",
    "    print(f\"  Result saved to: {result_path}\")\n",
    "    print(f\"  Database HLLSet ID: {ingestion_result['database']['data_id']}\")\n",
    "    print(f\"  Tables ingested: {len(ingestion_result['tables'])}\")\n",
    "else:\n",
    "    print(\"Database not found - cannot ingest\")\n",
    "    ingestion_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d58e0",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Query Helper\n",
    "\n",
    "Load the ingestion result and create query helper for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fe35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query helper initialized\n",
      "\n",
      "======================================================================\n",
      "DATABASE SUMMARY\n",
      "======================================================================\n",
      "Database HLLSet cardinality: 133,580\n",
      "Number of tables: 6\n",
      "Total columns: 72\n",
      "\n",
      "Tables:\n",
      "  • fdic_failed_banklist           (  7 columns)\n",
      "  • investment_company_series_class ( 20 columns)\n",
      "  • nfl_offensive_player_stats_1999_2013 ( 26 columns)\n",
      "  • public_company_bankruptcy_cases (  5 columns)\n",
      "  • subprime_2006_distributed      (  5 columns)\n",
      "  • top_teams_payroll              (  9 columns)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load ingestion result (if not already in memory)\n",
    "result_path = Path('./data/ingestion_result.json')\n",
    "\n",
    "if result_path.exists():\n",
    "    if 'ingestion_result' not in locals() or ingestion_result is None:\n",
    "        with open(result_path, 'r') as f:\n",
    "            ingestion_result = json.load(f)\n",
    "    \n",
    "    # Initialize query helper with database path for sample queries\n",
    "    helper = DatabaseQueryHelper(manifold, str(result_path), db_path=str(db_path))\n",
    "    \n",
    "    print(\"✓ Query helper initialized\")\n",
    "    print()\n",
    "    helper.print_database_summary()\n",
    "else:\n",
    "    print(\"Ingestion result not found - run ingestion first\")\n",
    "    helper = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd45941",
   "metadata": {},
   "source": [
    "## Step 5: Semantic Column Search\n",
    "\n",
    "Search for columns by semantic similarity, not just name matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581672d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for columns related to: ['revenue', 'sales', 'income', 'amount', 'total']\n",
      "\n",
      "Found 0 matches:\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Search for revenue/sales-related columns\n",
    "if helper:\n",
    "    search_keywords = [\"revenue\", \"sales\", \"income\", \"amount\", \"total\"]\n",
    "    \n",
    "    print(f\"Searching for columns related to: {search_keywords}\\n\")\n",
    "    \n",
    "    matches = helper.search_columns(search_keywords, threshold=0.1)\n",
    "    \n",
    "    print(f\"Found {len(matches)} matches:\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for match in matches[:15]:  # Show top 15\n",
    "        print(f\"{match['similarity']:.3f} | {match['table']:30s} | {match['column']:25s}\")\n",
    "        print(f\"       Type: {match['data_type']:15s} | Distinct: {match['distinct_count']:>10,}\")\n",
    "        print()\n",
    "    \n",
    "    if len(matches) > 15:\n",
    "        print(f\"... and {len(matches) - 15} more matches\")\n",
    "else:\n",
    "    print(\"Helper not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec96dd",
   "metadata": {},
   "source": [
    "### Try Your Own Search\n",
    "\n",
    "Modify the keywords below to search for different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733ec163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: ['customer', 'name', 'email']\n",
      "Found 0 matches:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom search - modify these keywords\n",
    "if helper:\n",
    "    custom_keywords = [\"customer\", \"name\", \"email\"]  # UPDATE THIS\n",
    "    \n",
    "    matches = helper.search_columns(custom_keywords, threshold=0.1)\n",
    "    \n",
    "    print(f\"Searching: {custom_keywords}\")\n",
    "    print(f\"Found {len(matches)} matches:\\n\")\n",
    "    \n",
    "    for match in matches[:10]:\n",
    "        print(f\"{match['similarity']:.3f} | {match['table']:25s}.{match['column']:20s}\")\n",
    "        print(f\"       {match['data_type']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d774f54",
   "metadata": {},
   "source": [
    "## Step 6: Explore Table and Column Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about a specific table\n",
    "if helper:\n",
    "    # List available tables\n",
    "    tables = helper.list_tables()\n",
    "    print(f\"Available tables ({len(tables)}):\")\n",
    "    for i, table in enumerate(tables[:20], 1):\n",
    "        print(f\"  {i:3d}. {table}\")\n",
    "    \n",
    "    if len(tables) > 20:\n",
    "        print(f\"  ... and {len(tables) - 20} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a specific table (update table name)\n",
    "if helper:\n",
    "    table_name = helper.list_tables()[0]  # First table, or specify name\n",
    "    \n",
    "    info = helper.get_table_info(table_name)\n",
    "    \n",
    "    print(f\"Table: {info['name']}\")\n",
    "    print(f\"Rows: {info['metadata']['row_count']:,}\")\n",
    "    print(f\"\\nColumns ({len(info['columns'])}):\\n\")\n",
    "    \n",
    "    for col in info['columns']:\n",
    "        col_info = helper.get_column_info(table_name, col)\n",
    "        meta = col_info['metadata']\n",
    "        print(f\"  • {col:30s} | {meta['data_type']:15s} | {col_info['cardinality']:>8,} distinct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdba91",
   "metadata": {},
   "source": [
    "## Step 7: Sample Data Retrieval\n",
    "\n",
    "Get sample values from columns via disambiguation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3339603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample values from a column\n",
    "if helper:\n",
    "    table_name = helper.list_tables()[0]  # Or specify\n",
    "    column_name = helper.get_table_info(table_name)['columns'][0]  # Or specify\n",
    "    \n",
    "    print(f\"Sample values from {table_name}.{column_name}:\\n\")\n",
    "    \n",
    "    samples = helper.get_column_sample_values(table_name, column_name, limit=10)\n",
    "    \n",
    "    for i, value in enumerate(samples, 1):\n",
    "        print(f\"  {i:2d}. {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6a1e4",
   "metadata": {},
   "source": [
    "## Step 8: Foreign Key Detection\n",
    "\n",
    "Find potential foreign key relationships by comparing column HLLSets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a647e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns related to a specific column (potential FKs)\n",
    "if helper:\n",
    "    # Look for ID columns that might have FK relationships\n",
    "    id_columns = []\n",
    "    for table_name in helper.list_tables()[:10]:  # Check first 10 tables\n",
    "        table_info = helper.get_table_info(table_name)\n",
    "        for col in table_info['columns']:\n",
    "            if 'id' in col.lower():\n",
    "                id_columns.append((table_name, col))\n",
    "    \n",
    "    print(f\"Found {len(id_columns)} ID columns\\n\")\n",
    "    \n",
    "    if id_columns:\n",
    "        # Analyze first ID column\n",
    "        table_name, column_name = id_columns[0]\n",
    "        \n",
    "        print(f\"Analyzing relationships for: {table_name}.{column_name}\\n\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        related = helper.find_related_columns(table_name, column_name, threshold=0.5)\n",
    "        \n",
    "        if related:\n",
    "            print(f\"Found {len(related)} potential relationships:\\n\")\n",
    "            for rel in related[:10]:\n",
    "                print(f\"{rel['similarity']:.3f} | {rel['table']:25s}.{rel['column']:20s}\")\n",
    "                print(f\"       Overlap: {rel['overlap_cardinality']:,} values\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No strong relationships found (try lower threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a7e41",
   "metadata": {},
   "source": [
    "## Step 9: Cross-Table Analysis\n",
    "\n",
    "Compare data patterns across tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate data across tables\n",
    "if helper and ingestion_result:\n",
    "    from core.hllset import HLLSet\n",
    "    \n",
    "    # Example: Look for email columns across tables\n",
    "    email_columns = []\n",
    "    \n",
    "    for table_name, table_data in list(ingestion_result['tables'].items())[:20]:\n",
    "        for col_name in table_data['columns'].keys():\n",
    "            if 'email' in col_name.lower() or 'mail' in col_name.lower():\n",
    "                email_columns.append((table_name, col_name, table_data['columns'][col_name]))\n",
    "    \n",
    "    if len(email_columns) >= 2:\n",
    "        print(f\"Found {len(email_columns)} email-related columns:\\n\")\n",
    "        \n",
    "        for table, col, _ in email_columns:\n",
    "            print(f\"  • {table}.{col}\")\n",
    "        \n",
    "        # Compare first two\n",
    "        if len(email_columns) >= 2:\n",
    "            t1, c1, d1 = email_columns[0]\n",
    "            t2, c2, d2 = email_columns[1]\n",
    "            \n",
    "            # Load HLLSets\n",
    "            hll1 = HLLSet.from_roaring(manifold.retrieve_artifact(d1['data_id']))\n",
    "            hll2 = HLLSet.from_roaring(manifold.retrieve_artifact(d2['data_id']))\n",
    "            \n",
    "            # Calculate overlap\n",
    "            intersection = hll1.intersect(hll2)\n",
    "            union = hll1.union(hll2)\n",
    "            \n",
    "            print(f\"\\nComparing {t1}.{c1} vs {t2}.{c2}:\")\n",
    "            print(f\"  • {t1}.{c1}: {hll1.cardinality():,} unique values\")\n",
    "            print(f\"  • {t2}.{c2}: {hll2.cardinality():,} unique values\")\n",
    "            print(f\"  • Overlap: {intersection.cardinality():,} values\")\n",
    "            print(f\"  • Jaccard similarity: {intersection.cardinality() / union.cardinality():.3f}\")\n",
    "    else:\n",
    "        print(\"Not enough email columns found for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f83d5",
   "metadata": {},
   "source": [
    "### Export Graph\n",
    "\n",
    "Save the graph in multiple formats for external analysis tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35b598",
   "metadata": {},
   "source": [
    "### Visualize Database Graph\n",
    "\n",
    "**Warning**: Large databases may take time to render. Consider visualizing subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789aae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph from database hierarchy\n",
    "if helper and ingestion_result:\n",
    "    from core.graph_visualizer import (\n",
    "        LatticeGraphBuilder,\n",
    "        LatticeVisualizer,\n",
    "        test_consistency\n",
    "    )\n",
    "    \n",
    "    print(\"Building property graph from database hierarchy...\")\n",
    "    \n",
    "    builder = LatticeGraphBuilder()\n",
    "    builder.from_database_hierarchy(ingestion_result, manifold)\n",
    "    \n",
    "    # Test consistency\n",
    "    consistency = test_consistency(builder)\n",
    "    print(f\"\\nConsistency check: {'✓ PASSED' if consistency['passed'] else '✗ FAILED'}\")\n",
    "    \n",
    "    if consistency['errors']:\n",
    "        print(f\"Errors: {len(consistency['errors'])}\")\n",
    "        for error in consistency['errors'][:5]:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    if consistency['warnings']:\n",
    "        print(f\"Warnings: {len(consistency['warnings'])}\")\n",
    "    \n",
    "    # Get statistics\n",
    "    viz = LatticeVisualizer(builder)\n",
    "    viz.print_statistics()\n",
    "    \n",
    "    # Detect foreign keys\n",
    "    fk_count = builder.detect_potential_foreign_keys(threshold=0.7)\n",
    "    print(f\"\\n✓ Detected {fk_count} potential foreign key relationships\")\n",
    "else:\n",
    "    print(\"No ingestion result - skip visualization\")\n",
    "    builder = None\n",
    "    viz = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize database structure\n",
    "if viz:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    node_count = builder.graph.number_of_nodes()\n",
    "    \n",
    "    if node_count > 100:\n",
    "        print(f\"Graph has {node_count} nodes - showing subgraph instead...\")\n",
    "        \n",
    "        # Get first table and visualize its neighborhood\n",
    "        table_nodes = [nid for nid, meta in builder.nodes.items() if meta.node_type == 'table']\n",
    "        if table_nodes:\n",
    "            first_table = table_nodes[0]\n",
    "            print(f\"Showing subgraph around: {builder.nodes[first_table].label}\")\n",
    "            \n",
    "            viz.plot_subgraph(\n",
    "                node_ids=[first_table],\n",
    "                depth=2,\n",
    "                figsize=(14, 10),\n",
    "                layout='spring',\n",
    "                show_labels=True\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Visualizing full graph ({node_count} nodes)...\")\n",
    "        viz.plot(\n",
    "            figsize=(14, 10),\n",
    "            layout='spring',\n",
    "            show_labels=True\n",
    "        )\n",
    "else:\n",
    "    print(\"No graph to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246587b",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Database Structure\n",
    "\n",
    "Use graph visualization to see the database hierarchy as a property graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16d0cd",
   "metadata": {},
   "source": [
    "## Step 10: Query Planning\n",
    "\n",
    "Use HLLSet cardinality to estimate query selectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca223871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate query result size\n",
    "if helper and ingestion_result:\n",
    "    from core.hllset import HLLSet\n",
    "    \n",
    "    # Example: Estimate \"WHERE column IN (values)\" selectivity\n",
    "    table_name = helper.list_tables()[0]\n",
    "    table_info = helper.get_table_info(table_name)\n",
    "    column_name = table_info['columns'][0]\n",
    "    \n",
    "    # Get column HLLSet\n",
    "    col_data = ingestion_result['tables'][table_name]['columns'][column_name]\n",
    "    col_hllset = HLLSet.from_roaring(manifold.retrieve_artifact(col_data['data_id']))\n",
    "    \n",
    "    # Create query HLLSet (simulate WHERE clause)\n",
    "    # Get some sample values to use as query filter\n",
    "    sample_values = helper.get_column_sample_values(table_name, column_name, limit=5)\n",
    "    query_hllset = HLLSet.from_batch(sample_values[:3])  # Use 3 values\n",
    "    \n",
    "    # Estimate selectivity\n",
    "    matching = col_hllset.intersect(query_hllset)\n",
    "    selectivity = matching.cardinality() / col_hllset.cardinality()\n",
    "    estimated_rows = int(table_info['metadata']['row_count'] * selectivity)\n",
    "    \n",
    "    print(f\"Query estimation for: {table_name}.{column_name}\")\n",
    "    print(f\"\\nQuery filter: WHERE {column_name} IN {sample_values[:3]}\")\n",
    "    print(f\"\\nEstimates:\")\n",
    "    print(f\"  • Table rows: {table_info['metadata']['row_count']:,}\")\n",
    "    print(f\"  • Distinct values in column: {col_hllset.cardinality():,}\")\n",
    "    print(f\"  • Matching values: {matching.cardinality():,}\")\n",
    "    print(f\"  • Selectivity: {selectivity:.3%}\")\n",
    "    print(f\"  • Estimated result rows: {estimated_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29956b29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "✅ Converted CSV files → DuckDB  \n",
    "✅ Ingested database column-by-column into HLLSets  \n",
    "✅ Created data + metadata hierarchies  \n",
    "✅ Performed semantic column search  \n",
    "✅ Detected foreign key relationships  \n",
    "✅ Analyzed cross-table patterns  \n",
    "✅ Estimated query selectivity  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Natural Language → SQL**: Use column search to help generate SQL queries\n",
    "- **Data Quality**: Find duplicate/inconsistent data across tables\n",
    "- **Schema Evolution**: Track changes over time\n",
    "- **Privacy**: Data stored as HLLSets, requires disambiguation to access\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [DOCS/COLUMNAR_INGESTION.md](DOCS/COLUMNAR_INGESTION.md) - Full architecture\n",
    "- [tools/README_CSV2DB.md](tools/README_CSV2DB.md) - CSV converter details\n",
    "- [QUICKSTART_COLUMNAR.md](QUICKSTART_COLUMNAR.md) - Quick reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
