{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f88eea",
   "metadata": {},
   "source": [
    "# DuckDB Metadata Bridge Playground\n",
    "\n",
    "Interactive exploration of the Enterprise-to-AI metadata bridge using DuckDB persistent storage via the ManifoldOS Extension System.\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **DuckDB Basics** - Create, query, and inspect databases\n",
    "2. **Extension System** - How DuckDB integrates as an extension\n",
    "3. **LUT Storage** - Persist and retrieve lookup tables\n",
    "4. **Metadata Bridge** - Two-way ED↔AI grounding\n",
    "5. **Advanced Queries** - Analytics on metadata\n",
    "6. **Performance** - Benchmark operations\n",
    "\n",
    "**Note**: This notebook now uses the ManifoldOS Extension System. DuckDB storage is a pluggable extension that can be replaced with PostgreSQL, Redis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da28b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/alexmy/SGS/SGS_lib/hllset_manifold\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a08ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extension system available\n",
      "  Storage extension: DuckDBStorageExtension\n",
      "✓ DuckDB available: 1.4.4\n"
     ]
    }
   ],
   "source": [
    "# Check extension system\n",
    "from core.extensions import ExtensionRegistry, DuckDBStorageExtension\n",
    "\n",
    "print(\"✓ Extension system available\")\n",
    "print(f\"  Storage extension: {DuckDBStorageExtension.__name__}\")\n",
    "\n",
    "# Check if DuckDB is installed\n",
    "try:\n",
    "    import duckdb\n",
    "    print(f\"✓ DuckDB available: {duckdb.__version__}\")\n",
    "    DUCKDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"✗ DuckDB not installed (pip install duckdb)\")\n",
    "    DUCKDB_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69165faf",
   "metadata": {},
   "source": [
    "## 0. Extension System Overview\n",
    "\n",
    "Before diving into DuckDB specifics, let's understand the extension architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe20efd",
   "metadata": {},
   "source": [
    "## 1. DuckDB Basics\n",
    "\n",
    "Let's start with basic DuckDB operations to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07b1028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "  (1, 'alpha', 3.14)\n",
      "  (2, 'beta', 2.71)\n",
      "\n",
      "✓ DuckDB basics working!\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    # Create in-memory database\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    \n",
    "    # Create a simple table\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE sample (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            name TEXT,\n",
    "            value DOUBLE\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert data\n",
    "    conn.execute(\"\"\"\n",
    "        INSERT INTO sample VALUES\n",
    "            (1, 'alpha', 3.14),\n",
    "            (2, 'beta', 2.71),\n",
    "            (3, 'gamma', 1.41)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Query\n",
    "    result = conn.execute(\"SELECT * FROM sample WHERE value > 2\").fetchall()\n",
    "    print(\"Query results:\")\n",
    "    for row in result:\n",
    "        print(f\"  {row}\")\n",
    "    \n",
    "    print(\"\\n✓ DuckDB basics working!\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available - skipping demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08540aa3",
   "metadata": {},
   "source": [
    "## 2. LUT Store Implementation\n",
    "\n",
    "Now let's explore our persistent LUT storage layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LUT Store initialized\n",
      "  Database: :memory:\n",
      "\n",
      "Tables created: ['lut_records']\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    from core.lut_store import DuckDBLUTStore, LUTRecord\n",
    "    import json\n",
    "    \n",
    "    # Create in-memory LUT store\n",
    "    store = DuckDBLUTStore(':memory:')\n",
    "    \n",
    "    print(\"✓ LUT Store initialized\")\n",
    "    print(f\"  Database: {store.db_path}\")\n",
    "    \n",
    "    # Inspect schema\n",
    "    tables = store.conn.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'main'\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    print(f\"\\nTables created: {[t[0] for t in tables]}\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103599ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample LUT Record (initialized with data):\n",
      "  (reg, zeros) = (42, 3)\n",
      "  Tokens: [('customer',), ('premium',), ('revenue',)]\n",
      "  Hashes: {12345, 67890, 11111}\n",
      "\n",
      "Alternative: Start empty and add entries:\n",
      "  Added 2 entries: [('customer',), ('premium',)]\n",
      "\n",
      "Serialized to dict with 4 keys\n",
      "  Keys: ['reg', 'zeros', 'hashes', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    # Create sample LUT record\n",
    "    # Note: LUTRecord only contains (reg, zeros, hashes, tokens)\n",
    "    # The n and hllset_hash are passed to commit_lut() instead\n",
    "    \n",
    "    # Method 1: Initialize with data\n",
    "    sample_record = LUTRecord(\n",
    "        reg=42,\n",
    "        zeros=3,\n",
    "        hashes={12345, 67890, 11111},  # Set of token hashes\n",
    "        tokens=[(\"customer\",), (\"premium\",), (\"revenue\",)]\n",
    "    )\n",
    "    \n",
    "    print(\"Sample LUT Record (initialized with data):\")\n",
    "    print(f\"  (reg, zeros) = ({sample_record.reg}, {sample_record.zeros})\")\n",
    "    print(f\"  Tokens: {sample_record.tokens}\")\n",
    "    print(f\"  Hashes: {sample_record.hashes}\")\n",
    "    \n",
    "    # Method 2: Initialize empty and add entries (handles collisions)\n",
    "    print(\"\\nAlternative: Start empty and add entries:\")\n",
    "    empty_record = LUTRecord(reg=42, zeros=3)\n",
    "    empty_record.add_entry(12345, (\"customer\",))\n",
    "    empty_record.add_entry(67890, (\"premium\",))\n",
    "    print(f\"  Added 2 entries: {empty_record.tokens}\")\n",
    "    \n",
    "    # Convert to dict (for serialization)\n",
    "    record_dict = sample_record.to_dict()\n",
    "    print(f\"\\nSerialized to dict with {len(record_dict)} keys\")\n",
    "    print(f\"  Keys: {list(record_dict.keys())}\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608abad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LUT committed: 0 records inserted/updated\n",
      "\n",
      "Storage stats:\n",
      "  Total token hashes: 0\n",
      "  N-groups: {}\n",
      "  Oldest record: None\n",
      "  Newest record: None\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    # Commit LUT to database\n",
    "    # Note: n is passed as parameter (n-gram size: 1, 2, 3)\n",
    "    # hllset_hash and metadata are now ignored (new schema v2)\n",
    "    \n",
    "    num_records = store.commit_lut(\n",
    "        n=1,  # N-gram size (1-token, 2-token, 3-token)\n",
    "        lut={(42, 3): sample_record},  # Dict of (reg, zeros) -> LUTRecord\n",
    "        hllset_hash=\"test_hash_12345\",  # Ignored in new schema\n",
    "        metadata=None  # Ignored in new schema\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ LUT committed: {num_records} records inserted/updated\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = store.get_stats()\n",
    "    print(f\"\\nStorage stats:\")\n",
    "    print(f\"  Total token hashes: {stats['total_token_hashes']}\")\n",
    "    print(f\"  N-groups: {stats['n_groups']}\")\n",
    "    print(f\"  Oldest record: {stats['oldest_record']}\")\n",
    "    print(f\"  Newest record: {stats['newest_record']}\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bd351",
   "metadata": {},
   "source": [
    "### Hash Collision Handling\n",
    "\n",
    "When multiple tokens hash to the same `(reg, zeros)` coordinates, they're stored together in the same LUTRecord. This is how we handle collisions - the tokens array acts like a set (no duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c19e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating Hash Collision Handling\n",
      "\n",
      "Initial state: (reg=100, zeros=5)\n",
      "  Hashes: set()\n",
      "  Tokens: []\n",
      "\n",
      "Adding tokens that hash to same (reg, zeros):\n",
      "\n",
      "  1. Added hash=12345, token=('customer',)\n",
      "     → Hashes: 1, Tokens: 1\n",
      "  2. Added hash=67890, token=('premium',)\n",
      "     → Hashes: 2, Tokens: 2\n",
      "  3. Added hash=11111, token=('enterprise',)\n",
      "     → Hashes: 3, Tokens: 3\n",
      "  4. Added hash=12345, token=('customer',)\n",
      "     → Hashes: 3, Tokens: 3\n",
      "  5. Added hash=99999, token=('customer',)\n",
      "     → Hashes: 4, Tokens: 3\n",
      "\n",
      "Final state:\n",
      "  Unique hashes: [11111, 12345, 67890, 99999]\n",
      "  Unique tokens: [('customer',), ('premium',), ('enterprise',)]\n",
      "\n",
      "✓ Collisions handled correctly!\n",
      "  • Duplicate hashes automatically deduplicated (set)\n",
      "  • Duplicate tokens manually deduplicated (list)\n",
      "  • Same token with different hash: both stored\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    print(\"Demonstrating Hash Collision Handling\\n\")\n",
    "    \n",
    "    # Create a LUTRecord for a specific (reg, zeros)\n",
    "    collision_record = LUTRecord(reg=100, zeros=5)\n",
    "    \n",
    "    print(f\"Initial state: (reg={collision_record.reg}, zeros={collision_record.zeros})\")\n",
    "    print(f\"  Hashes: {collision_record.hashes}\")\n",
    "    print(f\"  Tokens: {collision_record.tokens}\\n\")\n",
    "    \n",
    "    # Simulate multiple tokens hashing to the same location\n",
    "    # In reality, these would come from HLLSet hashing\n",
    "    print(\"Adding tokens that hash to same (reg, zeros):\\n\")\n",
    "    \n",
    "    colliding_entries = [\n",
    "        (12345, (\"customer\",)),\n",
    "        (67890, (\"premium\",)),\n",
    "        (11111, (\"enterprise\",)),\n",
    "        (12345, (\"customer\",)),  # Duplicate - should be ignored\n",
    "        (99999, (\"customer\",)),  # Same token, different hash\n",
    "    ]\n",
    "    \n",
    "    for i, (hash_val, token) in enumerate(colliding_entries, 1):\n",
    "        collision_record.add_entry(hash_val, token)\n",
    "        print(f\"  {i}. Added hash={hash_val}, token={token}\")\n",
    "        print(f\"     → Hashes: {len(collision_record.hashes)}, Tokens: {len(collision_record.tokens)}\")\n",
    "    \n",
    "    print(f\"\\nFinal state:\")\n",
    "    print(f\"  Unique hashes: {sorted(collision_record.hashes)}\")\n",
    "    print(f\"  Unique tokens: {collision_record.tokens}\")\n",
    "    print(f\"\\n✓ Collisions handled correctly!\")\n",
    "    print(f\"  • Duplicate hashes automatically deduplicated (set)\")\n",
    "    print(f\"  • Duplicate tokens manually deduplicated (list)\")\n",
    "    print(f\"  • Same token with different hash: both stored\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3401ac",
   "metadata": {},
   "source": [
    "## 3. Metadata Bridge with ManifoldOS\n",
    "\n",
    "Now let's use the full metadata bridge through ManifoldOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8365b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ ManifoldOS initialized with LUT store\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    from core.manifold_os import ManifoldOS\n",
    "    \n",
    "    # Create ManifoldOS with in-memory LUT store\n",
    "    os = ManifoldOS(lut_db_path=':memory:')\n",
    "    \n",
    "    print(\"✓ ManifoldOS initialized with LUT store\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available - using ManifoldOS without persistence\")\n",
    "    from core.manifold_os import ManifoldOS\n",
    "    os = ManifoldOS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3636b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting enterprise data:\n",
      "\n",
      "  ✓ LUT committed: n=1, hash=981720c2cd45bbd0..., id=5\n",
      "  ✓ LUT committed: n=2, hash=8c0f8fc95157ebfb..., id=4\n",
      "  ✓ LUT committed: n=3, hash=34b00b646d0213ae..., id=3\n",
      "1. premium customer revenue growth engageme | 981720c2cd45...\n",
      "  ✓ LUT committed: n=1, hash=cd4bae22e33cd324..., id=5\n",
      "  ✓ LUT committed: n=2, hash=65faa6c912396d06..., id=4\n",
      "  ✓ LUT committed: n=3, hash=794dc8bccc43d3fe..., id=3\n",
      "2. enterprise software cloud platform subsc | cd4bae22e33c...\n",
      "  ✓ LUT committed: n=1, hash=29355f37f9d5179c..., id=5\n",
      "  ✓ LUT committed: n=2, hash=ef5d1ef2fb7ee37a..., id=4\n",
      "  ✓ LUT committed: n=3, hash=6e2355fdc8b28f7d..., id=3\n",
      "3. employee performance metrics analytics d | 29355f37f9d5...\n",
      "\n",
      "✓ Ingested 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Ingest enterprise data\n",
    "# Note: With new LUT schema, tokens are global (not per-HLLSet)\n",
    "\n",
    "enterprise_data = [\n",
    "    \"premium customer revenue growth engagement\",\n",
    "    \"enterprise software cloud platform subscription\", \n",
    "    \"employee performance metrics analytics dashboard\",\n",
    "]\n",
    "\n",
    "representations = []\n",
    "hllset_hashes = []\n",
    "\n",
    "print(\"Ingesting enterprise data:\\n\")\n",
    "for i, data in enumerate(enterprise_data, 1):\n",
    "    # Ingest returns NTokenRepresentation\n",
    "    rep = os.ingest(data)\n",
    "    \n",
    "    if rep and hasattr(rep, 'hllsets') and rep.hllsets and 1 in rep.hllsets:\n",
    "        representations.append(rep)\n",
    "        hash_val = rep.hllsets[1].name\n",
    "        hllset_hashes.append(hash_val)\n",
    "        print(f\"{i}. {data[:40]:40} | {hash_val[:12]}...\")\n",
    "    else:\n",
    "        print(f\"{i}. {data[:40]:40} | ⚠ No valid representation\")\n",
    "\n",
    "if representations:\n",
    "    print(f\"\\n✓ Ingested {len(representations)} datasets\")\n",
    "else:\n",
    "    print(f\"\\n⚠ No datasets successfully ingested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988cd79",
   "metadata": {},
   "source": [
    "## 4. Bidirectional Queries\n",
    "\n",
    "### 4a. AI → ED Grounding (Query tokens from coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1490e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI → Enterprise Data Grounding\n",
      "\n",
      "Scenario: AI has coordinates from HLLSet, needs source tokens\n",
      "\n",
      "HLLSet: 981720c2cd45bbd0b496...\n",
      "\n",
      "  (reg=101, zeros=0) → [('premium',)]\n",
      "  (reg= 76, zeros=0) → [('customer',)]\n",
      "  (reg=234, zeros=1) → [('revenue',)]\n",
      "\n",
      "✓ AI operations grounded to source tokens\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE and os.lut_store and representations:\n",
    "    print(\"AI → Enterprise Data Grounding\\n\")\n",
    "    print(\"Scenario: AI has coordinates from HLLSet, needs source tokens\\n\")\n",
    "    \n",
    "    # Get some sample coordinates from first representation\n",
    "    rep = representations[0]\n",
    "    \n",
    "    if hasattr(rep, 'luts') and rep.luts and 1 in rep.luts:\n",
    "        lut = rep.luts[1]\n",
    "        sample_keys = list(lut.keys())[:3]\n",
    "        hllset_hash = rep.hllsets[1].name\n",
    "        \n",
    "        print(f\"HLLSet: {hllset_hash[:20]}...\\n\")\n",
    "        \n",
    "        for reg, zeros in sample_keys:\n",
    "            tokens = os.query_tokens_from_metadata(\n",
    "                n=1,\n",
    "                reg=reg,\n",
    "                zeros=zeros,\n",
    "                hllset_hash=hllset_hash\n",
    "            )\n",
    "            print(f\"  (reg={reg:3}, zeros={zeros}) → {tokens}\")\n",
    "        \n",
    "        print(\"\\n✓ AI operations grounded to source tokens\")\n",
    "    else:\n",
    "        print(\"⚠ No LUT available in representation\")\n",
    "else:\n",
    "    print(\"⚠ LUT store not available or no data ingested\")\n",
    "    if not DUCKDB_AVAILABLE:\n",
    "        print(\"  Install DuckDB: pip install duckdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2782ff",
   "metadata": {},
   "source": [
    "### 4b. ED → AI Lookup (Find coordinates for tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b607c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enterprise Data → AI Coordinates\n",
      "\n",
      "Scenario: Enterprise has token, needs AI space location\n",
      "\n",
      "  Token customer     → Found at 1 coordinates\n",
      "    (reg= 76, zeros=0)\n",
      "  Token enterprise   → Found at 1 coordinates\n",
      "    (reg=322, zeros=0)\n",
      "  Token employee     → Found at 1 coordinates\n",
      "    (reg=468, zeros=0)\n",
      "\n",
      "✓ Enterprise tokens mapped to AI space\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE and os.lut_store:\n",
    "    print(\"Enterprise Data → AI Coordinates\\n\")\n",
    "    print(\"Scenario: Enterprise has token, needs AI space location\\n\")\n",
    "    \n",
    "    # Find where specific tokens appear\n",
    "    search_tokens = [('customer',), ('enterprise',), ('employee',)]\n",
    "    \n",
    "    for token in search_tokens:\n",
    "        keys = os.query_by_token(n=1, token_tuple=token)\n",
    "        if keys:\n",
    "            print(f\"  Token {token[0]:12} → Found at {len(keys)} coordinates\")\n",
    "            # Show first few\n",
    "            for reg, zeros in keys[:2]:\n",
    "                print(f\"    (reg={reg:3}, zeros={zeros})\")\n",
    "        else:\n",
    "            print(f\"  Token {token[0]:12} → Not found\")\n",
    "    \n",
    "    print(\"\\n✓ Enterprise tokens mapped to AI space\")\n",
    "else:\n",
    "    print(\"⚠ LUT store not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705b8f5",
   "metadata": {},
   "source": [
    "### 4c. Metadata Retrieval (Audit trail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7933073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata & Audit Trail\n",
      "\n",
      "✓ Full provenance tracking enabled\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE and os.lut_store:\n",
    "    print(\"Metadata & Audit Trail\\n\")\n",
    "    \n",
    "    for i, hash_val in enumerate(hllset_hashes, 1):\n",
    "        metadata = os.get_ingestion_metadata(hash_val)\n",
    "        \n",
    "        if metadata:\n",
    "            print(f\"{i}. HLLSet: {hash_val[:16]}...\")\n",
    "            print(f\"   Source: {metadata.get('source', 'unknown')}\")\n",
    "            print(f\"   Table: {metadata.get('table', 'unknown')}\")\n",
    "            print(f\"   Record ID: {metadata.get('record_id', 'unknown')}\")\n",
    "            print(f\"   Tokens: {metadata.get('original_length', 'unknown')}\")\n",
    "            print()\n",
    "    \n",
    "    print(\"✓ Full provenance tracking enabled\")\n",
    "else:\n",
    "    print(\"⚠ LUT store not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1a010",
   "metadata": {},
   "source": [
    "## 5. Advanced SQL Queries\n",
    "\n",
    "DuckDB gives us full SQL power for analytics on metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7d7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent Storage Statistics\n",
      "\n",
      "Total token hashes: 36\n",
      "\n",
      "Records by N-gram:\n",
      "  n=1: 15\n",
      "  n=2: 12\n",
      "  n=3: 9\n",
      "\n",
      "Oldest record: 2026-02-07 18:26:58.182599\n",
      "Newest record: 2026-02-07 18:26:58.354808\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE and os.lut_store:\n",
    "    stats = os.get_lut_stats()\n",
    "    \n",
    "    print(\"Persistent Storage Statistics\\n\")\n",
    "    print(f\"Total token hashes: {stats['total_token_hashes']}\")\n",
    "    print(f\"\\nRecords by N-gram:\")\n",
    "    for n, count in stats['n_groups'].items():\n",
    "        print(f\"  n={n}: {count}\")\n",
    "    \n",
    "    if stats['oldest_record']:\n",
    "        print(f\"\\nOldest record: {stats['oldest_record']}\")\n",
    "        print(f\"Newest record: {stats['newest_record']}\")\n",
    "else:\n",
    "    print(\"⚠ LUT store not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2472157",
   "metadata": {},
   "source": [
    "## 6. Storage Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf98bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent Storage Statistics\n",
      "\n",
      "Total token hashes: 36\n",
      "\n",
      "Records by N-gram:\n",
      "  n=1: 15\n",
      "  n=2: 12\n",
      "  n=3: 9\n",
      "\n",
      "Oldest record: 2026-02-07 18:26:58.182599\n",
      "Newest record: 2026-02-07 18:26:58.354808\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE and os.lut_store:\n",
    "    stats = os.get_lut_stats()\n",
    "    \n",
    "    print(\"Persistent Storage Statistics\\n\")\n",
    "    print(f\"Total token hashes: {stats['total_token_hashes']}\")\n",
    "    print(f\"\\nRecords by N-gram:\")\n",
    "    for n, count in stats['n_groups'].items():\n",
    "        print(f\"  n={n}: {count}\")\n",
    "    \n",
    "    if stats['oldest_record']:\n",
    "        print(f\"\\nOldest record: {stats['oldest_record']}\")\n",
    "        print(f\"Newest record: {stats['newest_record']}\")\n",
    "else:\n",
    "    print(\"⚠ LUT store not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2113c",
   "metadata": {},
   "source": [
    "## 7. Persistent Storage Demo\n",
    "\n",
    "Show that data persists across sessions using a file-based database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2283336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temporary database: /tmp/tmpesedeeuv.duckdb\n",
      "\n",
      "[Session 1] Writing data...\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "  ✓ LUT committed: n=1, hash=702d0b0d750a5006..., id=4\n",
      "  ✓ LUT committed: n=2, hash=15cdd0c0c950ae3f..., id=3\n",
      "  ✓ LUT committed: n=3, hash=4bb6926a60356c1b..., id=2\n",
      "  Wrote 9 token hashes\n",
      "  HLLSet: 702d0b0d750a50062f2f...\n",
      "\n",
      "[Session 2] Reading persisted data...\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "  Found 9 token hashes (persisted!)\n",
      "\n",
      "✓ Persistence verified!\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    import tempfile\n",
    "    import os as os_module\n",
    "    \n",
    "    # Create temporary database file\n",
    "    # Note: We need to delete the empty file first so DuckDB can create a fresh database\n",
    "    with tempfile.NamedTemporaryFile(suffix='.duckdb', delete=False) as f:\n",
    "        temp_db = f.name\n",
    "    \n",
    "    # Delete the empty file - DuckDB will create a new database\n",
    "    os_module.remove(temp_db)\n",
    "    \n",
    "    print(f\"Using temporary database: {temp_db}\\n\")\n",
    "    \n",
    "    # Session 1: Write data\n",
    "    print(\"[Session 1] Writing data...\")\n",
    "    os1 = ManifoldOS(lut_db_path=temp_db)\n",
    "    rep1 = os1.ingest(\n",
    "        \"persistent metadata example test\",\n",
    "        metadata={'session': 1, 'note': 'first write'}\n",
    "    )\n",
    "    \n",
    "    if rep1 and hasattr(rep1, 'hllsets') and rep1.hllsets and 1 in rep1.hllsets:\n",
    "        saved_hash = rep1.hllsets[1].name\n",
    "        stats1 = os1.get_lut_stats()\n",
    "        print(f\"  Wrote {stats1['total_token_hashes']} token hashes\")\n",
    "        print(f\"  HLLSet: {saved_hash[:20]}...\")\n",
    "        \n",
    "        if os1.lut_store:\n",
    "            os1.lut_store.close()\n",
    "        \n",
    "        # Session 2: Read data\n",
    "        print(\"\\n[Session 2] Reading persisted data...\")\n",
    "        os2 = ManifoldOS(lut_db_path=temp_db)\n",
    "        stats2 = os2.get_lut_stats()\n",
    "        print(f\"  Found {stats2['total_token_hashes']} token hashes (persisted!)\")\n",
    "        \n",
    "        metadata = os2.get_ingestion_metadata(saved_hash)\n",
    "        if metadata:\n",
    "            print(f\"  Metadata: {metadata}\")\n",
    "        \n",
    "        if os2.lut_store:\n",
    "            os2.lut_store.close()\n",
    "        \n",
    "        # Cleanup\n",
    "        os_module.remove(temp_db)\n",
    "        \n",
    "        print(\"\\n✓ Persistence verified!\")\n",
    "    else:\n",
    "        print(\"⚠ Ingestion failed - no valid representation created\")\n",
    "        if os1.lut_store:\n",
    "            os1.lut_store.close()\n",
    "        os_module.remove(temp_db)\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8207b3",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c1de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Benchmarks\n",
      "\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "  ✓ LUT committed: n=1, hash=e9924d2f7c5748c8..., id=7\n",
      "  ✓ LUT committed: n=2, hash=6992081fef45e42a..., id=6\n",
      "  ✓ LUT committed: n=3, hash=4505cd1ac10d0f1a..., id=5\n",
      "  ✓ LUT committed: n=1, hash=cc9e5d4f213a1517..., id=7\n",
      "  ✓ LUT committed: n=2, hash=ff903ae177e672d3..., id=6\n",
      "  ✓ LUT committed: n=3, hash=bc50771b59353c6d..., id=5\n",
      "  ✓ LUT committed: n=1, hash=fd02eaa8725179c0..., id=7\n",
      "  ✓ LUT committed: n=2, hash=c8214aabe6b01096..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ac0cb13160c20df5..., id=5\n",
      "  ✓ LUT committed: n=1, hash=054af13e200708a9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=27cd95c98089be66..., id=6\n",
      "  ✓ LUT committed: n=3, hash=0a0e980f61e40f1c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=c97fd0ed7add6439..., id=7\n",
      "  ✓ LUT committed: n=2, hash=1bbaa0b285413f43..., id=6\n",
      "  ✓ LUT committed: n=3, hash=9cfd4907864767ba..., id=5\n",
      "  ✓ LUT committed: n=1, hash=b20692bd54f72541..., id=7\n",
      "  ✓ LUT committed: n=2, hash=35bbfb33aa5454f0..., id=6\n",
      "  ✓ LUT committed: n=3, hash=c83296de9e2b5236..., id=5\n",
      "  ✓ LUT committed: n=1, hash=41ef0c80ef832625..., id=7\n",
      "  ✓ LUT committed: n=2, hash=e41b5ef182e0d25e..., id=6\n",
      "  ✓ LUT committed: n=3, hash=25d50051e63638bf..., id=5\n",
      "  ✓ LUT committed: n=1, hash=f7a1b9ac7f9b549c..., id=7\n",
      "  ✓ LUT committed: n=2, hash=fc4c104619b686a2..., id=6\n",
      "  ✓ LUT committed: n=3, hash=91aea3e8cb0a10ec..., id=5\n",
      "  ✓ LUT committed: n=1, hash=72d20056c26d453c..., id=7\n",
      "  ✓ LUT committed: n=2, hash=745e0a2a64376886..., id=6\n",
      "  ✓ LUT committed: n=3, hash=e0186fc13772db27..., id=5\n",
      "  ✓ LUT committed: n=1, hash=3d0f25ed558674d4..., id=7\n",
      "  ✓ LUT committed: n=2, hash=838ec4e76356bab0..., id=6\n",
      "  ✓ LUT committed: n=3, hash=0e154f50c545d227..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9086b10a58429a57..., id=7\n",
      "  ✓ LUT committed: n=2, hash=9516a3585c13e030..., id=6\n",
      "  ✓ LUT committed: n=3, hash=690aa14b17d2ad31..., id=5\n",
      "  ✓ LUT committed: n=1, hash=12f2fddb871fdaca..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4abe0d1739c2d182..., id=6\n",
      "  ✓ LUT committed: n=3, hash=47ddbff14ebefbae..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9b96c600bb389476..., id=7\n",
      "  ✓ LUT committed: n=2, hash=ce09c0ba13914057..., id=6\n",
      "  ✓ LUT committed: n=3, hash=0cea860fffa1fb95..., id=5\n",
      "  ✓ LUT committed: n=1, hash=2eea18ac53244b26..., id=7\n",
      "  ✓ LUT committed: n=2, hash=97ecdda01cbc9d4f..., id=6\n",
      "  ✓ LUT committed: n=3, hash=459b022cdf0e51ca..., id=5\n",
      "  ✓ LUT committed: n=1, hash=b5ec6071b33aa9f1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=2c2bd9a1d3d704c8..., id=6\n",
      "  ✓ LUT committed: n=3, hash=66eee6e7c1a762e6..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0f088a8b115effd1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=0f92bda6455183ba..., id=6\n",
      "  ✓ LUT committed: n=3, hash=7063e4cb25872eb8..., id=5\n",
      "  ✓ LUT committed: n=1, hash=fe31037a4678638f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=92b5634a072e0bfb..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d4adfb046979939d..., id=5\n",
      "  ✓ LUT committed: n=1, hash=38e5a58cc732db16..., id=7\n",
      "  ✓ LUT committed: n=2, hash=1c943e7bb00a4eb1..., id=6\n",
      "  ✓ LUT committed: n=3, hash=59c2b42a96154fc8..., id=5\n",
      "  ✓ LUT committed: n=1, hash=45ed45f4e7e84999..., id=7\n",
      "  ✓ LUT committed: n=2, hash=2751cb772f0f7565..., id=6\n",
      "  ✓ LUT committed: n=3, hash=03ef10a56c4459b9..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4f68a2d69555dbae..., id=7\n",
      "  ✓ LUT committed: n=2, hash=d2d8fef6398107fc..., id=6\n",
      "  ✓ LUT committed: n=3, hash=444b1762f5f1ce26..., id=5\n",
      "  ✓ LUT committed: n=1, hash=b47788767e04a2e0..., id=7\n",
      "  ✓ LUT committed: n=2, hash=5525a677fe434b0d..., id=6\n",
      "  ✓ LUT committed: n=3, hash=775b4fd6c0f3486f..., id=5\n",
      "  ✓ LUT committed: n=1, hash=da9e6e49d50e5ae1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=14d936ad076cc9c4..., id=6\n",
      "  ✓ LUT committed: n=3, hash=30ba541eedf0475a..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0f90ea5cee18ffbe..., id=7\n",
      "  ✓ LUT committed: n=2, hash=59c8476a567df397..., id=6\n",
      "  ✓ LUT committed: n=3, hash=e10716b2637d969c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=66140a43f02ebae6..., id=7\n",
      "  ✓ LUT committed: n=2, hash=2a90b611be46d220..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ef3319dc6997dcc2..., id=5\n",
      "  ✓ LUT committed: n=1, hash=ef2ddeb8a71a2abf..., id=7\n",
      "  ✓ LUT committed: n=2, hash=3d10d804c05daed3..., id=6\n",
      "  ✓ LUT committed: n=3, hash=60681088ad669503..., id=5\n",
      "  ✓ LUT committed: n=1, hash=056939f1504a48b1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=76d65096adda4430..., id=6\n",
      "  ✓ LUT committed: n=3, hash=cd9ae4d6f6f5d7c9..., id=5\n",
      "  ✓ LUT committed: n=1, hash=27b216b03528a474..., id=7\n",
      "  ✓ LUT committed: n=2, hash=7fbab5d6d1fbdb2a..., id=6\n",
      "  ✓ LUT committed: n=3, hash=8d3d42874e9abc63..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4e92b452d96baf1f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=19d3f81a88d064df..., id=6\n",
      "  ✓ LUT committed: n=3, hash=7e31bcf45f7bb625..., id=5\n",
      "  ✓ LUT committed: n=1, hash=c64165465d80afae..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4612ed60e09ab7d8..., id=6\n",
      "  ✓ LUT committed: n=3, hash=e40af00cd1bf4ca5..., id=5\n",
      "  ✓ LUT committed: n=1, hash=2bff007d87ca4e30..., id=7\n",
      "  ✓ LUT committed: n=2, hash=04fcef51934e9d6e..., id=6\n",
      "  ✓ LUT committed: n=3, hash=f26afd149f36ed2c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0eb09cde3e4aa619..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4d79771603cf5e14..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ebc74a6eb2fb1402..., id=5\n",
      "  ✓ LUT committed: n=1, hash=785b37e0caa08d3b..., id=7\n",
      "  ✓ LUT committed: n=2, hash=a5200eb2fb0be8f5..., id=6\n",
      "  ✓ LUT committed: n=3, hash=4cb54aea548ceef2..., id=5\n",
      "  ✓ LUT committed: n=1, hash=190e4b15f34bdfd1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=503d659d9c224944..., id=6\n",
      "  ✓ LUT committed: n=3, hash=2cac62fa091c4501..., id=5\n",
      "  ✓ LUT committed: n=1, hash=d7c096ccea145013..., id=7\n",
      "  ✓ LUT committed: n=2, hash=bcc41500c0f322db..., id=6\n",
      "  ✓ LUT committed: n=3, hash=a7c2d54d5e2d53b0..., id=5\n",
      "  ✓ LUT committed: n=1, hash=84a2bbac44423a6f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=3c391ac311ed9ebd..., id=6\n",
      "  ✓ LUT committed: n=3, hash=40aca95157720fa6..., id=5\n",
      "  ✓ LUT committed: n=1, hash=12d2f6026ed2278d..., id=7\n",
      "  ✓ LUT committed: n=2, hash=9a2285b123994e69..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d63376eedb2bc855..., id=5\n",
      "  ✓ LUT committed: n=1, hash=c917e9e4edd280e8..., id=7\n",
      "  ✓ LUT committed: n=2, hash=517b4004a48f24c7..., id=6\n",
      "  ✓ LUT committed: n=3, hash=58b76cf4a782caf4..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9ce5e69e694bedc1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=866af9c1778b6fa2..., id=6\n",
      "  ✓ LUT committed: n=3, hash=41f6f36f117517df..., id=5\n",
      "  ✓ LUT committed: n=1, hash=af596b711794b616..., id=7\n",
      "  ✓ LUT committed: n=2, hash=093916c39d49a856..., id=6\n",
      "  ✓ LUT committed: n=3, hash=c912c71def839835..., id=5\n",
      "  ✓ LUT committed: n=1, hash=ca7f925345a5ea0e..., id=7\n",
      "  ✓ LUT committed: n=2, hash=6f3b9ecfcf6a4deb..., id=6\n",
      "  ✓ LUT committed: n=3, hash=6b73fd0cfab7cdbc..., id=5\n",
      "  ✓ LUT committed: n=1, hash=1fba7515a3a78ee9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=7cd3a403fdbf1234..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b5e381a0329b7014..., id=5\n",
      "  ✓ LUT committed: n=1, hash=5c13a0cea34847b4..., id=7\n",
      "  ✓ LUT committed: n=2, hash=e15383fdd00a3b55..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ca3cc23ae184e929..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4c72977fc5348213..., id=7\n",
      "  ✓ LUT committed: n=2, hash=730a249909abd11a..., id=6\n",
      "  ✓ LUT committed: n=3, hash=1fcce77180c49240..., id=5\n",
      "  ✓ LUT committed: n=1, hash=a48c0fa2afb06e38..., id=7\n",
      "  ✓ LUT committed: n=2, hash=79d1279932145056..., id=6\n",
      "  ✓ LUT committed: n=3, hash=296cf8ef025323b1..., id=5\n",
      "  ✓ LUT committed: n=1, hash=062d3fae67a0ada9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=11ecd369b7849763..., id=6\n",
      "  ✓ LUT committed: n=3, hash=04131a993303c0af..., id=5\n",
      "  ✓ LUT committed: n=1, hash=99ae1ce59a8bfc3d..., id=7\n",
      "  ✓ LUT committed: n=2, hash=9cff73212e3405d4..., id=6\n",
      "  ✓ LUT committed: n=3, hash=08c48374c3773918..., id=5\n",
      "  ✓ LUT committed: n=1, hash=81b948a69e663602..., id=7\n",
      "  ✓ LUT committed: n=2, hash=16476af57bc14bfd..., id=6\n",
      "  ✓ LUT committed: n=3, hash=7907c03a37f23c2e..., id=5\n",
      "  ✓ LUT committed: n=1, hash=7f3082dcf2f117d0..., id=7\n",
      "  ✓ LUT committed: n=2, hash=b56e49b825191a65..., id=6\n",
      "  ✓ LUT committed: n=3, hash=45e69dd3114ff3ba..., id=5\n",
      "  ✓ LUT committed: n=1, hash=748198e1d3a4e2bd..., id=7\n",
      "  ✓ LUT committed: n=2, hash=f500c21314473de6..., id=6\n",
      "  ✓ LUT committed: n=3, hash=2c79ccf39250be44..., id=5\n",
      "  ✓ LUT committed: n=1, hash=68810fc4bb09d235..., id=7\n",
      "  ✓ LUT committed: n=2, hash=1f453d7fb41c2c08..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d3d4f9071ee09465..., id=5\n",
      "  ✓ LUT committed: n=1, hash=a11265aa52307464..., id=7\n",
      "  ✓ LUT committed: n=2, hash=de56929a379a9c7b..., id=6\n",
      "  ✓ LUT committed: n=3, hash=8bd3a426d4d67c8a..., id=5\n",
      "  ✓ LUT committed: n=1, hash=18d1844f28d5ecb9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=8b6f8fac1a58b5a7..., id=6\n",
      "  ✓ LUT committed: n=3, hash=1770bf2728147e47..., id=5\n",
      "  ✓ LUT committed: n=1, hash=40d83dacb6f61aca..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4fedb73c39a6281a..., id=6\n",
      "  ✓ LUT committed: n=3, hash=67f4f05a228da042..., id=5\n",
      "  ✓ LUT committed: n=1, hash=505ee410a35f9fa1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=7d0c93060090f9db..., id=6\n",
      "  ✓ LUT committed: n=3, hash=6c8cc46f7e482c8d..., id=5\n",
      "  ✓ LUT committed: n=1, hash=f5aa9dcab32454d2..., id=7\n",
      "  ✓ LUT committed: n=2, hash=e28cc7d6e9dcb7a3..., id=6\n",
      "  ✓ LUT committed: n=3, hash=80f15ee0e3302bd0..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0176efb328529f0c..., id=7\n",
      "  ✓ LUT committed: n=2, hash=5cb8a10375f65352..., id=6\n",
      "  ✓ LUT committed: n=3, hash=61100b5419598204..., id=5\n",
      "  ✓ LUT committed: n=1, hash=f9f1909e48c7a014..., id=7\n",
      "  ✓ LUT committed: n=2, hash=35c4cc23e7b6aaa2..., id=6\n",
      "  ✓ LUT committed: n=3, hash=e818b0cd6cc05e4e..., id=5\n",
      "  ✓ LUT committed: n=1, hash=d5d22ded1601a725..., id=7\n",
      "  ✓ LUT committed: n=2, hash=3ef25d94ef2bcabc..., id=6\n",
      "  ✓ LUT committed: n=3, hash=91c1242443197e41..., id=5\n",
      "  ✓ LUT committed: n=1, hash=02d3447e03fbdfc2..., id=7\n",
      "  ✓ LUT committed: n=2, hash=48b830f88cf8f16f..., id=6\n",
      "  ✓ LUT committed: n=3, hash=64cd416646e0da4f..., id=5\n",
      "  ✓ LUT committed: n=1, hash=61bbd84f814d33c7..., id=7\n",
      "  ✓ LUT committed: n=2, hash=7d58fc949dc4fafc..., id=6\n",
      "  ✓ LUT committed: n=3, hash=791502b94f756a57..., id=5\n",
      "  ✓ LUT committed: n=1, hash=19bfe61daa94e7a3..., id=7\n",
      "  ✓ LUT committed: n=2, hash=f22349dd71a4d165..., id=6\n",
      "  ✓ LUT committed: n=3, hash=7f918aaf77f982e9..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0330bb5cda6246c3..., id=7\n",
      "  ✓ LUT committed: n=2, hash=eb89e8fd87f19620..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b1173ad4ce66cce8..., id=5\n",
      "  ✓ LUT committed: n=1, hash=6247507b30c9565f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=44a14550961796ed..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d5cf0608d68c2622..., id=5\n",
      "  ✓ LUT committed: n=1, hash=e427f94bf5a8aba9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=afcd7fbb75d55b2b..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d341f5dd4fad5aaf..., id=5\n",
      "  ✓ LUT committed: n=1, hash=3dc1224e25abe2f9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=d6c26bbf77b07377..., id=6\n",
      "  ✓ LUT committed: n=3, hash=895680466e716bfe..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4719443fa9f11f8f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=535150489491fddd..., id=6\n",
      "  ✓ LUT committed: n=3, hash=35e41fa1126e5b88..., id=5\n",
      "  ✓ LUT committed: n=1, hash=2101877a83dbff32..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4c67e084bbc55c8e..., id=6\n",
      "  ✓ LUT committed: n=3, hash=5885ed1068ee0938..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4b79206da9a25d8f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=87373af8cd4e2ed4..., id=6\n",
      "  ✓ LUT committed: n=3, hash=2c9f144fed9299e7..., id=5\n",
      "  ✓ LUT committed: n=1, hash=a4d25940eef4bfd2..., id=7\n",
      "  ✓ LUT committed: n=2, hash=120e95b7b7190b09..., id=6\n",
      "  ✓ LUT committed: n=3, hash=5a32f0758bda197f..., id=5\n",
      "  ✓ LUT committed: n=1, hash=0fd11f83c5ac6b0f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=ed33306cd215a474..., id=6\n",
      "  ✓ LUT committed: n=3, hash=064b2aafe502b5cc..., id=5\n",
      "  ✓ LUT committed: n=1, hash=5e519407956cb46e..., id=7\n",
      "  ✓ LUT committed: n=2, hash=8a747f83d948d402..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ac7f7807132641d5..., id=5\n",
      "  ✓ LUT committed: n=1, hash=e7479fcf32cf37c2..., id=7\n",
      "  ✓ LUT committed: n=2, hash=8789b8193965ed83..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b5b23f9eb0ec5e6c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=416b430854008d8f..., id=7\n",
      "  ✓ LUT committed: n=2, hash=22b6628f3fafedab..., id=6\n",
      "  ✓ LUT committed: n=3, hash=9f71a63fd1abae2f..., id=5\n",
      "  ✓ LUT committed: n=1, hash=09314101c1385107..., id=7\n",
      "  ✓ LUT committed: n=2, hash=807a3ebf79b4d98e..., id=6\n",
      "  ✓ LUT committed: n=3, hash=25dd94fc57d5aa0c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=a128c95ab9bfc336..., id=7\n",
      "  ✓ LUT committed: n=2, hash=dcec4dc1d00af65e..., id=6\n",
      "  ✓ LUT committed: n=3, hash=28b19bc7906af3f6..., id=5\n",
      "  ✓ LUT committed: n=1, hash=c6e85a2df20bfaed..., id=7\n",
      "  ✓ LUT committed: n=2, hash=09df0f6296998520..., id=6\n",
      "  ✓ LUT committed: n=3, hash=18ba87676585f2ba..., id=5\n",
      "  ✓ LUT committed: n=1, hash=6cb52a262d96c4a6..., id=7\n",
      "  ✓ LUT committed: n=2, hash=c306794f71f98012..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b46205467ffca7ea..., id=5\n",
      "  ✓ LUT committed: n=1, hash=d16932c67bdb45f3..., id=7\n",
      "  ✓ LUT committed: n=2, hash=633001097575eed7..., id=6\n",
      "  ✓ LUT committed: n=3, hash=490a9098a6662054..., id=5\n",
      "  ✓ LUT committed: n=1, hash=3d0331812153b3a1..., id=7\n",
      "  ✓ LUT committed: n=2, hash=3e4f6d931f6a484d..., id=6\n",
      "  ✓ LUT committed: n=3, hash=8818ba2991ef48cc..., id=5\n",
      "  ✓ LUT committed: n=1, hash=748198e1d3a4e2bd..., id=7\n",
      "  ✓ LUT committed: n=2, hash=bc5ebd786365893c..., id=6\n",
      "  ✓ LUT committed: n=3, hash=ea17057986fd49a8..., id=5\n",
      "  ✓ LUT committed: n=1, hash=45fdc4a1f8bd53a7..., id=7\n",
      "  ✓ LUT committed: n=2, hash=b5467832b6fdcb13..., id=6\n",
      "  ✓ LUT committed: n=3, hash=cf41156f86b64047..., id=5\n",
      "  ✓ LUT committed: n=1, hash=87531028fa9791ae..., id=7\n",
      "  ✓ LUT committed: n=2, hash=28c21b954f6f2ed3..., id=6\n",
      "  ✓ LUT committed: n=3, hash=1196cf916c992929..., id=5\n",
      "  ✓ LUT committed: n=1, hash=d7a670798d6ae710..., id=7\n",
      "  ✓ LUT committed: n=2, hash=2ca281369014a006..., id=6\n",
      "  ✓ LUT committed: n=3, hash=820c336532a9d505..., id=5\n",
      "  ✓ LUT committed: n=1, hash=2789d8027e22c0bd..., id=7\n",
      "  ✓ LUT committed: n=2, hash=9b874af8dfb42eea..., id=6\n",
      "  ✓ LUT committed: n=3, hash=02483ca2c8cb462b..., id=5\n",
      "  ✓ LUT committed: n=1, hash=f566576c2b961f60..., id=7\n",
      "  ✓ LUT committed: n=2, hash=99fce665b91ca1f8..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b8909aa8ad11058e..., id=5\n",
      "  ✓ LUT committed: n=1, hash=dc52508c8aec0103..., id=7\n",
      "  ✓ LUT committed: n=2, hash=7f7ed2b9557bc0bd..., id=6\n",
      "  ✓ LUT committed: n=3, hash=d687c53c53ff3def..., id=5\n",
      "  ✓ LUT committed: n=1, hash=b9bdf2d58b03b3ab..., id=7\n",
      "  ✓ LUT committed: n=2, hash=4a1a9be23f7447d2..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b7cd5596db18ad7e..., id=5\n",
      "  ✓ LUT committed: n=1, hash=047b321ef83b2d6c..., id=7\n",
      "  ✓ LUT committed: n=2, hash=6e9797c3454b7e8d..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b7a063cfa632868c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=4d9b0ebd07fc4e69..., id=7\n",
      "  ✓ LUT committed: n=2, hash=1bdd7ba18ef2f8b8..., id=6\n",
      "  ✓ LUT committed: n=3, hash=cb6071e64e2e5e13..., id=5\n",
      "  ✓ LUT committed: n=1, hash=cb58bcaafa0983b9..., id=7\n",
      "  ✓ LUT committed: n=2, hash=dbc424815209d3ea..., id=6\n",
      "  ✓ LUT committed: n=3, hash=649baa0cac7749cc..., id=5\n",
      "  ✓ LUT committed: n=1, hash=d29ebbce485b3532..., id=7\n",
      "  ✓ LUT committed: n=2, hash=ed9b2637dad5fb22..., id=6\n",
      "  ✓ LUT committed: n=3, hash=3402edc3d9162497..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9a0f8fc869f9fdf8..., id=7\n",
      "  ✓ LUT committed: n=2, hash=cd533bfdd14da0dc..., id=6\n",
      "  ✓ LUT committed: n=3, hash=baa77a4f0c4dd05f..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9fc9b9f7b1e2df5d..., id=7\n",
      "  ✓ LUT committed: n=2, hash=af72a8a4fee95f44..., id=6\n",
      "  ✓ LUT committed: n=3, hash=693cacd92f84531c..., id=5\n",
      "  ✓ LUT committed: n=1, hash=2becacd9d1aba039..., id=7\n",
      "  ✓ LUT committed: n=2, hash=02a9755f3a1027b4..., id=6\n",
      "  ✓ LUT committed: n=3, hash=3a05bd1c9c7cc0d3..., id=5\n",
      "  ✓ LUT committed: n=1, hash=9796c992afa2a679..., id=7\n",
      "  ✓ LUT committed: n=2, hash=e90d7b14f6c33d1a..., id=6\n",
      "  ✓ LUT committed: n=3, hash=9fc0b97e2c085c7d..., id=5\n",
      "  ✓ LUT committed: n=1, hash=90fd472999388276..., id=7\n",
      "  ✓ LUT committed: n=2, hash=76c469815f070928..., id=6\n",
      "  ✓ LUT committed: n=3, hash=5bb10e67c747d5b0..., id=5\n",
      "  ✓ LUT committed: n=1, hash=bfd8bf9fb9ac039c..., id=7\n",
      "  ✓ LUT committed: n=2, hash=130358e8ed0f7892..., id=6\n",
      "  ✓ LUT committed: n=3, hash=b9b38c9b212f07d6..., id=5\n",
      "  ✓ LUT committed: n=1, hash=65d02746a6590d72..., id=7\n",
      "  ✓ LUT committed: n=2, hash=846712881b8d49a1..., id=6\n",
      "  ✓ LUT committed: n=3, hash=e4bd0dc46bddd040..., id=5\n",
      "  ✓ LUT committed: n=1, hash=ec56de440f3c6e22..., id=7\n",
      "  ✓ LUT committed: n=2, hash=8b7c51f8b008cd04..., id=6\n",
      "  ✓ LUT committed: n=3, hash=2f3f99edc0680aae..., id=5\n",
      "  ✓ LUT committed: n=1, hash=702b2d4861ffd1bb..., id=7\n",
      "  ✓ LUT committed: n=2, hash=146d1fc5819c0291..., id=6\n",
      "  ✓ LUT committed: n=3, hash=05ae1713ee191c2b..., id=5\n",
      "Ingestion: 100 documents in 8.860s\n",
      "  Throughput: 11.3 docs/sec\n",
      "\n",
      "Query stats: 1000 calls in 2.610s\n",
      "  Throughput: 383.2 queries/sec\n",
      "\n",
      "✓ Benchmarks complete\n"
     ]
    }
   ],
   "source": [
    "if DUCKDB_AVAILABLE:\n",
    "    import time\n",
    "    \n",
    "    print(\"Performance Benchmarks\\n\")\n",
    "    \n",
    "    # Benchmark 1: Ingestion speed\n",
    "    os_bench = ManifoldOS(lut_db_path=':memory:')\n",
    "    \n",
    "    test_data = [\n",
    "        f\"sample text number {i} with content data\"\n",
    "        for i in range(100)\n",
    "    ]\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, text in enumerate(test_data):\n",
    "        os_bench.ingest(text, metadata={'batch_id': i})\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Ingestion: {len(test_data)} documents in {elapsed:.3f}s\")\n",
    "    print(f\"  Throughput: {len(test_data)/elapsed:.1f} docs/sec\")\n",
    "    \n",
    "    # Benchmark 2: Query speed\n",
    "    if os_bench.lut_store:\n",
    "        start = time.time()\n",
    "        for _ in range(1000):\n",
    "            stats = os_bench.get_lut_stats()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"\\nQuery stats: 1000 calls in {elapsed:.3f}s\")\n",
    "        print(f\"  Throughput: {1000/elapsed:.1f} queries/sec\")\n",
    "    \n",
    "    print(\"\\n✓ Benchmarks complete\")\n",
    "else:\n",
    "    print(\"⚠ DuckDB not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc29c78",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "1. ✅ **DuckDB Integration** - Embedded SQL database with ACID guarantees\n",
    "2. ✅ **LUT Persistence** - Store and retrieve lookup tables\n",
    "3. ✅ **Metadata Bridge** - Two-way ED↔AI grounding\n",
    "4. ✅ **SQL Analytics** - Rich queries on metadata\n",
    "5. ✅ **Persistence** - Data survives across sessions\n",
    "6. ✅ **Performance** - Fast ingestion and queries\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Explainability**: Trace AI decisions to source data\n",
    "- **Compliance**: Full audit trails for regulations\n",
    "- **Grounding**: Bridge abstract AI to concrete enterprise reality\n",
    "- **Analytics**: SQL power for metadata exploration\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with your own data\n",
    "- Try different SQL queries\n",
    "- Explore Redis/PostgreSQL backends\n",
    "- Build production workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
