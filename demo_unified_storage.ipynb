{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74bf5ad",
   "metadata": {},
   "source": [
    "# Unified Storage Extension Demo\n",
    "\n",
    "This notebook demonstrates the **Unified Storage Extension** - a multi-perceptron storage system with:\n",
    "- **Unified lattice model**: AM, W, and metadata graphs all represented as lattice instances\n",
    "- **Roaring bitmap compression**: 6-50x compression for HLLSets\n",
    "- **Multi-perceptron support**: Different hash functions and configurations per perceptron\n",
    "- **Content-addressable**: IICA compliant (Immutable, Idempotent, Content Addressable)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         Unified Storage Extension                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Perceptrons Registry                            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ data_perceptron_1 (SHA1, seed=42)            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ metadata_perceptron_1 (SHA1, seed=99)        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ image_perceptron_1 (xxHash, seed=123)        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Unified Lattice Tables                          ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ lattices (typed by perceptron)               ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ lattice_nodes (JSON properties)              ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ lattice_edges (JSON properties)              ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ hllsets (Roaring compressed)                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Entanglement Layer                              ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ entanglements (cross-perceptron)             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ entanglement_mappings (œÜ: L‚ÇÅ ‚Üí L‚ÇÇ)           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477a99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from core.extensions.unified_storage import UnifiedStorageExtension, PerceptronConfig\n",
    "from core.hllset import HLLSet\n",
    "from core.hrt import HRT\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a175074",
   "metadata": {},
   "source": [
    "## 1. Initialize Unified Storage\n",
    "\n",
    "Create an in-memory database with unified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23136a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Storage initialized\n",
      "Available: True\n",
      "Capabilities: multi_perceptron, roaring_compression, unified_lattice, am_storage, w_storage, metadata_storage, entanglement_storage, content_addressable\n"
     ]
    }
   ],
   "source": [
    "# Create unified storage\n",
    "storage = UnifiedStorageExtension(\":memory:\")\n",
    "\n",
    "print(\"Unified Storage initialized\")\n",
    "print(f\"Available: {storage.is_available()}\")\n",
    "print(f\"Capabilities: {', '.join(storage.get_capabilities())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc82c83",
   "metadata": {},
   "source": [
    "## 2. Register Multiple Perceptrons\n",
    "\n",
    "Each perceptron can have different configurations:\n",
    "- **Data perceptron**: Process text data with n-token algorithm\n",
    "- **Metadata perceptron**: Process database metadata (tables, columns, FKs)\n",
    "- **Image perceptron**: (Future) Process images with different hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f09d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered perceptrons:\n",
      "  data_perceptron_1: data (sha1, seed=42)\n",
      "  metadata_perceptron_1: metadata (sha1, seed=99)\n"
     ]
    }
   ],
   "source": [
    "# Register data perceptron\n",
    "data_config = PerceptronConfig(\n",
    "    perceptron_id=\"data_perceptron_1\",\n",
    "    perceptron_type=\"data\",\n",
    "    hash_function=\"sha1\",\n",
    "    hash_seed=42,\n",
    "    config_dict={\n",
    "        \"n_tokens\": 5,\n",
    "        \"p_bits\": 14,\n",
    "        \"description\": \"Text data processing\"\n",
    "    }\n",
    ")\n",
    "storage.register_perceptron(data_config, \"Data perceptron for text processing\")\n",
    "\n",
    "# Register metadata perceptron\n",
    "metadata_config = PerceptronConfig(\n",
    "    perceptron_id=\"metadata_perceptron_1\",\n",
    "    perceptron_type=\"metadata\",\n",
    "    hash_function=\"sha1\",\n",
    "    hash_seed=99,\n",
    "    config_dict={\n",
    "        \"schema_aware\": True,\n",
    "        \"p_bits\": 12,\n",
    "        \"description\": \"Database metadata processing\"\n",
    "    }\n",
    ")\n",
    "storage.register_perceptron(metadata_config, \"Metadata perceptron for database schemas\")\n",
    "\n",
    "print(\"Registered perceptrons:\")\n",
    "for p in storage.list_perceptrons():\n",
    "    print(f\"  {p['perceptron_id']}: {p['perceptron_type']} ({p['hash_function']}, seed={p['hash_seed']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5dfe6",
   "metadata": {},
   "source": [
    "## 3. Data Perceptron: Store AM and W Lattices\n",
    "\n",
    "Process text data and store both adjacency matrix and weight lattices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370c61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample data:\n",
      "  Unique tokens: 10\n",
      "  Transitions: 12\n",
      "  Tokens: ['brown', 'dog', 'fast', 'fox', 'jumps', 'lazy', 'over', 'quick', 'runs', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Create sample token data for AM lattice\n",
    "# In real usage, this would come from HRT or ManifoldOS ingestion\n",
    "\n",
    "# Sample tokens and their transitions (bigrams)\n",
    "text = \"the quick brown fox jumps over the lazy dog the fox runs fast\"\n",
    "tokens = text.split()\n",
    "\n",
    "# Build transitions\n",
    "from collections import defaultdict\n",
    "transitions = defaultdict(int)\n",
    "for i in range(len(tokens) - 1):\n",
    "    transitions[(tokens[i], tokens[i+1])] += 1\n",
    "\n",
    "# Create token to index mapping\n",
    "unique_tokens = sorted(set(tokens))\n",
    "token_to_idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "# Create token_lut (mapping indices to tokens)\n",
    "token_lut = {(idx, 0): token for token, idx in token_to_idx.items()}\n",
    "\n",
    "# Create AM cells\n",
    "am_cells = [\n",
    "    (token_to_idx[src], token_to_idx[tgt], freq)\n",
    "    for (src, tgt), freq in transitions.items()\n",
    "]\n",
    "\n",
    "print(f\"Created sample data:\")\n",
    "print(f\"  Unique tokens: {len(unique_tokens)}\")\n",
    "print(f\"  Transitions: {len(am_cells)}\")\n",
    "print(f\"  Tokens: {unique_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc28f9f",
   "metadata": {},
   "source": [
    "### 3.1 Store AM Lattice (Adjacency Matrix)\n",
    "\n",
    "AM lattice represents token transitions:\n",
    "- **Nodes**: Tokens (am_token type)\n",
    "- **Edges**: Transitions with frequency weights (am_transition type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1ed675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored AM lattice: 43d66c7c39455ab305c1ab822383d19594368229\n",
      "\n",
      "AM Lattice Info:\n",
      "  Type: AM\n",
      "  Dimension: 10\n",
      "  Nodes: 10 tokens\n",
      "  Edges: 12 transitions\n",
      "\n",
      "Top 5 transitions (by frequency):\n",
      "  the ‚Üí quick: 1x\n",
      "  quick ‚Üí brown: 1x\n",
      "  brown ‚Üí fox: 1x\n",
      "  fox ‚Üí jumps: 1x\n",
      "  jumps ‚Üí over: 1x\n"
     ]
    }
   ],
   "source": [
    "# Store AM lattice\n",
    "am_lattice_id = storage.store_am_lattice(\n",
    "    perceptron_id=\"data_perceptron_1\",\n",
    "    am_cells=am_cells,\n",
    "    token_lut=token_lut,\n",
    "    dimension=len(unique_tokens)\n",
    ")\n",
    "\n",
    "print(f\"Stored AM lattice: {am_lattice_id}\")\n",
    "\n",
    "# Query AM lattice\n",
    "am_info = storage.get_lattice_info(am_lattice_id)\n",
    "am_nodes = storage.get_lattice_nodes(am_lattice_id, node_type=\"am_token\")\n",
    "am_edges = storage.get_lattice_edges(am_lattice_id, edge_type=\"am_transition\")\n",
    "\n",
    "print(f\"\\nAM Lattice Info:\")\n",
    "print(f\"  Type: {am_info['lattice_type']}\")\n",
    "print(f\"  Dimension: {am_info['dimension']}\")\n",
    "print(f\"  Nodes: {len(am_nodes)} tokens\")\n",
    "print(f\"  Edges: {len(am_edges)} transitions\")\n",
    "\n",
    "# Show top transitions\n",
    "print(f\"\\nTop 5 transitions (by frequency):\")\n",
    "for edge in am_edges[:5]:\n",
    "    src = next(n for n in am_nodes if n['node_id'] == edge['source_node'])\n",
    "    tgt = next(n for n in am_nodes if n['node_id'] == edge['target_node'])\n",
    "    print(f\"  {src['properties']['token']} ‚Üí {tgt['properties']['token']}: {int(edge['weight'])}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813fa74",
   "metadata": {},
   "source": [
    "### 3.2 Store W Lattice (Weight/HLLSet Lattice)\n",
    "\n",
    "W lattice represents context relationships:\n",
    "- **Nodes**: HLLSets at different positions (w_hllset type)\n",
    "- **Edges**: Morphisms showing similarity (w_morphism type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7525eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 basic HLLSets\n",
      "Created 4 morphisms\n",
      "\n",
      "Stored W lattice: 98a6c5f6e35d3e8f92958ebc22bcd8acc9beb4c8\n",
      "\n",
      "W Lattice Info:\n",
      "  Type: W\n",
      "  Dimension: 5\n",
      "  Nodes: 5 HLLSets\n",
      "  Edges: 4 morphisms\n",
      "\n",
      "Top morphisms (by similarity):\n",
      "  group_3 ‚Üî group_4: 0.250\n",
      "  group_0 ‚Üî group_1: 0.200\n",
      "  group_1 ‚Üî group_2: 0.200\n",
      "  group_2 ‚Üî group_3: 0.200\n"
     ]
    }
   ],
   "source": [
    "# Create basic HLLSets from tokens (simulating W lattice)\n",
    "basic_hllsets = []\n",
    "\n",
    "# Group tokens into small sets to create HLLSets\n",
    "token_groups = [\n",
    "    unique_tokens[:3],   # Group 0\n",
    "    unique_tokens[2:5],  # Group 1 (overlap)\n",
    "    unique_tokens[4:7],  # Group 2\n",
    "    unique_tokens[6:9],  # Group 3\n",
    "    unique_tokens[8:],   # Group 4\n",
    "]\n",
    "\n",
    "for idx, token_group in enumerate(token_groups):\n",
    "    if token_group:  # Skip empty groups\n",
    "        hllset = HLLSet.from_batch(token_group, p_bits=14)\n",
    "        position = f\"group_{idx}\"\n",
    "        basic_hllsets.append((idx, position, hllset))\n",
    "\n",
    "print(f\"Created {len(basic_hllsets)} basic HLLSets\")\n",
    "\n",
    "# Create morphisms (compare HLLSets using Jaccard similarity)\n",
    "morphisms = []\n",
    "for i in range(len(basic_hllsets)):\n",
    "    for j in range(i+1, min(i+3, len(basic_hllsets))):  # Connect to next 2\n",
    "        hll_i = basic_hllsets[i][2]\n",
    "        hll_j = basic_hllsets[j][2]\n",
    "        \n",
    "        # Calculate Jaccard similarity (built-in method)\n",
    "        similarity = hll_i.similarity(hll_j)\n",
    "        \n",
    "        # Estimate intersection size for metadata\n",
    "        card_i = hll_i.cardinality()\n",
    "        card_j = hll_j.cardinality()\n",
    "        card_union = hll_i.union(hll_j).cardinality()\n",
    "        intersection = max(0, card_i + card_j - card_union)\n",
    "        \n",
    "        if similarity > 0.01:  # Only store significant similarities\n",
    "            morphisms.append((\n",
    "                i, j, similarity,\n",
    "                {\"type\": \"context_similarity\", \"intersection\": intersection}\n",
    "            ))\n",
    "\n",
    "print(f\"Created {len(morphisms)} morphisms\")\n",
    "\n",
    "# Store W lattice\n",
    "w_lattice_id = storage.store_w_lattice(\n",
    "    perceptron_id=\"data_perceptron_1\",\n",
    "    basic_hllsets=basic_hllsets,\n",
    "    morphisms=morphisms,\n",
    "    dimension=len(basic_hllsets)\n",
    ")\n",
    "\n",
    "print(f\"\\nStored W lattice: {w_lattice_id}\")\n",
    "\n",
    "# Query W lattice\n",
    "w_info = storage.get_lattice_info(w_lattice_id)\n",
    "w_nodes = storage.get_lattice_nodes(w_lattice_id, node_type=\"w_hllset\")\n",
    "w_edges = storage.get_lattice_edges(w_lattice_id, edge_type=\"w_morphism\")\n",
    "\n",
    "print(f\"\\nW Lattice Info:\")\n",
    "print(f\"  Type: {w_info['lattice_type']}\")\n",
    "print(f\"  Dimension: {w_info['dimension']}\")\n",
    "print(f\"  Nodes: {len(w_nodes)} HLLSets\")\n",
    "print(f\"  Edges: {len(w_edges)} morphisms\")\n",
    "\n",
    "# Show top morphisms\n",
    "print(f\"\\nTop morphisms (by similarity):\")\n",
    "for edge in w_edges[:min(5, len(w_edges))]:\n",
    "    src = next(n for n in w_nodes if n['node_id'] == edge['source_node'])\n",
    "    tgt = next(n for n in w_nodes if n['node_id'] == edge['target_node'])\n",
    "    print(f\"  {src['properties']['position']} ‚Üî {tgt['properties']['position']}: {edge['weight']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1a2fc",
   "metadata": {},
   "source": [
    "## 4. HLLSet Compression Statistics\n",
    "\n",
    "Roaring bitmap compression reduces storage by 6-50x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807387f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Storage Statistics:\n",
      "  Perceptrons: 2\n",
      "  Lattices: 2\n",
      "  Nodes: 15\n",
      "  Edges: 16\n",
      "  HLLSets: 5\n",
      "\n",
      "HLLSet Compression:\n",
      "  Original size: 81,920 bytes\n",
      "  Compressed size: 180 bytes\n",
      "  Average ratio: 461.80x\n",
      "  Space saved: 99.8%\n",
      "\n",
      "Sample HLLSet compression:\n",
      "  HLLSet 0 (group_0):\n",
      "    Cardinality: 3.0\n",
      "    Original: 16,384 bytes\n",
      "    Compressed: 38 bytes\n",
      "    Ratio: 431.00x\n",
      "  HLLSet 1 (group_1):\n",
      "    Cardinality: 3.0\n",
      "    Original: 16,384 bytes\n",
      "    Compressed: 38 bytes\n",
      "    Ratio: 431.00x\n",
      "  HLLSet 2 (group_2):\n",
      "    Cardinality: 3.0\n",
      "    Original: 16,384 bytes\n",
      "    Compressed: 38 bytes\n",
      "    Ratio: 431.00x\n"
     ]
    }
   ],
   "source": [
    "# Get compression statistics\n",
    "stats = storage.get_storage_stats()\n",
    "\n",
    "print(\"Overall Storage Statistics:\")\n",
    "print(f\"  Perceptrons: {stats['perceptrons']}\")\n",
    "print(f\"  Lattices: {stats['lattices']}\")\n",
    "print(f\"  Nodes: {stats['nodes']}\")\n",
    "print(f\"  Edges: {stats['edges']}\")\n",
    "print(f\"  HLLSets: {stats['hllsets']}\")\n",
    "print(f\"\\nHLLSet Compression:\")\n",
    "print(f\"  Original size: {stats['hllset_compression']['total_original']:,} bytes\")\n",
    "print(f\"  Compressed size: {stats['hllset_compression']['total_compressed']:,} bytes\")\n",
    "print(f\"  Average ratio: {stats['hllset_compression']['avg_compression_ratio']:.2f}x\")\n",
    "print(f\"  Space saved: {(1 - stats['hllset_compression']['total_compressed']/stats['hllset_compression']['total_original'])*100:.1f}%\")\n",
    "\n",
    "# Show individual HLLSet stats\n",
    "print(f\"\\nSample HLLSet compression:\")\n",
    "for i, (_, position, hllset) in enumerate(basic_hllsets[:3]):\n",
    "    hllset_stats = storage.get_hllset_stats(hllset.name)\n",
    "    if hllset_stats:\n",
    "        print(f\"  HLLSet {i} ({position}):\")\n",
    "        print(f\"    Cardinality: {hllset_stats['cardinality']:.1f}\")\n",
    "        print(f\"    Original: {hllset_stats['original_size']:,} bytes\")\n",
    "        print(f\"    Compressed: {hllset_stats['compressed_size']:,} bytes\")\n",
    "        print(f\"    Ratio: {hllset_stats['compression_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589bfdd",
   "metadata": {},
   "source": [
    "## 5. Metadata Perceptron: Database Schema as Lattice\n",
    "\n",
    "Demonstrate how database metadata can be represented as a lattice:\n",
    "- **Nodes**: Tables, columns (meta_table, meta_column types)\n",
    "- **Edges**: Foreign keys, dependencies (meta_fk, meta_dep types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4cc1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metadata lattice: c46cb930cfd098b71e18d26d54911e80e52e3454\n",
      "Added 4 table nodes\n",
      "Added 3 foreign key edges\n",
      "\n",
      "Metadata Lattice Info:\n",
      "  Type: metadata\n",
      "  Perceptron: metadata\n",
      "  Schema: ecommerce\n",
      "  Tables: 4\n",
      "  Foreign Keys: 3\n",
      "\n",
      "Schema Structure:\n",
      "  üìä customers (10,000 rows)\n",
      "     Columns: id, name, email\n",
      "  üìä orders (50,000 rows)\n",
      "     Columns: id, customer_id, total\n",
      "  üìä products (1,000 rows)\n",
      "     Columns: id, name, price\n",
      "  üìä order_items (150,000 rows)\n",
      "     Columns: id, order_id, product_id, quantity\n",
      "\n",
      "Foreign Key Relationships:\n",
      "  orders.customer_id ‚Üí customers (strength: 1.00)\n",
      "  order_items.order_id ‚Üí orders (strength: 0.95)\n",
      "  order_items.product_id ‚Üí products (strength: 0.80)\n"
     ]
    }
   ],
   "source": [
    "from core.extensions.unified_storage import LatticeNode, LatticeEdge\n",
    "from core.hllset import compute_sha1\n",
    "\n",
    "# Create metadata lattice\n",
    "metadata_lattice_id = storage.create_lattice(\n",
    "    perceptron_id=\"metadata_perceptron_1\",\n",
    "    lattice_type=\"metadata\",\n",
    "    dimension=10,\n",
    "    config={\"schema\": \"ecommerce\", \"database\": \"main\"}\n",
    ")\n",
    "\n",
    "print(f\"Created metadata lattice: {metadata_lattice_id}\")\n",
    "\n",
    "# Add table nodes\n",
    "tables = [\n",
    "    {\"name\": \"customers\", \"columns\": [\"id\", \"name\", \"email\"], \"rows\": 10000},\n",
    "    {\"name\": \"orders\", \"columns\": [\"id\", \"customer_id\", \"total\"], \"rows\": 50000},\n",
    "    {\"name\": \"products\", \"columns\": [\"id\", \"name\", \"price\"], \"rows\": 1000},\n",
    "    {\"name\": \"order_items\", \"columns\": [\"id\", \"order_id\", \"product_id\", \"quantity\"], \"rows\": 150000},\n",
    "]\n",
    "\n",
    "table_nodes = {}\n",
    "for idx, table in enumerate(tables):\n",
    "    node_id = compute_sha1(f\"meta_table:metadata_perceptron_1:{table['name']}\")\n",
    "    \n",
    "    node = LatticeNode(\n",
    "        node_id=node_id,\n",
    "        node_index=idx,\n",
    "        node_type=\"meta_table\",\n",
    "        content_hash=compute_sha1(table['name']),\n",
    "        cardinality=float(table['rows']),\n",
    "        properties={\n",
    "            \"table_name\": table['name'],\n",
    "            \"columns\": table['columns'],\n",
    "            \"row_count\": table['rows']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    storage.store_lattice_node(metadata_lattice_id, node)\n",
    "    table_nodes[table['name']] = node_id\n",
    "\n",
    "print(f\"Added {len(tables)} table nodes\")\n",
    "\n",
    "# Add foreign key edges\n",
    "foreign_keys = [\n",
    "    (\"orders\", \"customers\", \"customer_id\", 1.0),\n",
    "    (\"order_items\", \"orders\", \"order_id\", 0.95),\n",
    "    (\"order_items\", \"products\", \"product_id\", 0.8),\n",
    "]\n",
    "\n",
    "for src_table, tgt_table, fk_column, strength in foreign_keys:\n",
    "    edge_id = compute_sha1(f\"meta_fk:{src_table}:{tgt_table}:{fk_column}\")\n",
    "    \n",
    "    edge = LatticeEdge(\n",
    "        edge_id=edge_id,\n",
    "        source_node=table_nodes[src_table],\n",
    "        target_node=table_nodes[tgt_table],\n",
    "        edge_type=\"meta_fk\",\n",
    "        weight=strength,\n",
    "        properties={\n",
    "            \"fk_column\": fk_column,\n",
    "            \"constraint_type\": \"foreign_key\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    storage.store_lattice_edge(metadata_lattice_id, edge)\n",
    "\n",
    "print(f\"Added {len(foreign_keys)} foreign key edges\")\n",
    "\n",
    "# Query metadata lattice\n",
    "meta_info = storage.get_lattice_info(metadata_lattice_id)\n",
    "meta_nodes = storage.get_lattice_nodes(metadata_lattice_id, node_type=\"meta_table\")\n",
    "meta_edges = storage.get_lattice_edges(metadata_lattice_id, edge_type=\"meta_fk\")\n",
    "\n",
    "print(f\"\\nMetadata Lattice Info:\")\n",
    "print(f\"  Type: {meta_info['lattice_type']}\")\n",
    "print(f\"  Perceptron: {meta_info['perceptron_type']}\")\n",
    "print(f\"  Schema: {meta_info['config']['schema']}\")\n",
    "print(f\"  Tables: {len(meta_nodes)}\")\n",
    "print(f\"  Foreign Keys: {len(meta_edges)}\")\n",
    "\n",
    "print(f\"\\nSchema Structure:\")\n",
    "for node in meta_nodes:\n",
    "    table_name = node['properties']['table_name']\n",
    "    row_count = node['properties']['row_count']\n",
    "    columns = \", \".join(node['properties']['columns'])\n",
    "    print(f\"  üìä {table_name} ({row_count:,} rows)\")\n",
    "    print(f\"     Columns: {columns}\")\n",
    "\n",
    "print(f\"\\nForeign Key Relationships:\")\n",
    "for edge in meta_edges:\n",
    "    src = next(n for n in meta_nodes if n['node_id'] == edge['source_node'])\n",
    "    tgt = next(n for n in meta_nodes if n['node_id'] == edge['target_node'])\n",
    "    fk_col = edge['properties']['fk_column']\n",
    "    print(f\"  {src['properties']['table_name']}.{fk_col} ‚Üí {tgt['properties']['table_name']} (strength: {edge['weight']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1968f7",
   "metadata": {},
   "source": [
    "## 6. Multi-Perceptron Architecture\n",
    "\n",
    "The unified storage now contains lattices from **two different perceptrons**:\n",
    "1. **Data perceptron**: Text processing (AM + W lattices)\n",
    "2. **Metadata perceptron**: Database schema (metadata lattice)\n",
    "\n",
    "Each perceptron maintains its own hash morphism consistency while sharing the same storage infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec273b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Perceptron Storage Summary:\n",
      "============================================================\n",
      "\n",
      "DATA PERCEPTRON: data_perceptron_1\n",
      "  Hash: sha1 (seed=42)\n",
      "  Description: Data perceptron for text processing\n",
      "  Lattices: 2\n",
      "    ‚Ä¢ AM lattice: 10 nodes, 12 edges, dim=10\n",
      "    ‚Ä¢ W lattice: 5 nodes, 4 edges, dim=5\n",
      "\n",
      "METADATA PERCEPTRON: metadata_perceptron_1\n",
      "  Hash: sha1 (seed=99)\n",
      "  Description: Metadata perceptron for database schemas\n",
      "  Lattices: 1\n",
      "    ‚Ä¢ metadata lattice: 4 nodes, 3 edges, dim=10\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Unified storage successfully handles multiple perceptrons!\n"
     ]
    }
   ],
   "source": [
    "# Summary of multi-perceptron storage\n",
    "all_perceptrons = storage.list_perceptrons()\n",
    "\n",
    "print(\"Multi-Perceptron Storage Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for perceptron in all_perceptrons:\n",
    "    perc_id = perceptron['perceptron_id']\n",
    "    \n",
    "    # Get lattices for this perceptron\n",
    "    lattices = storage.conn.execute(\"\"\"\n",
    "        SELECT lattice_id, lattice_type, dimension\n",
    "        FROM lattices WHERE perceptron_id = ?\n",
    "    \"\"\", [perc_id]).fetchall()\n",
    "    \n",
    "    print(f\"\\n{perceptron['perceptron_type'].upper()} PERCEPTRON: {perc_id}\")\n",
    "    print(f\"  Hash: {perceptron['hash_function']} (seed={perceptron['hash_seed']})\")\n",
    "    print(f\"  Description: {perceptron['description']}\")\n",
    "    print(f\"  Lattices: {len(lattices)}\")\n",
    "    \n",
    "    for lattice_id, lattice_type, dimension in lattices:\n",
    "        node_count = storage.conn.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM lattice_nodes WHERE lattice_id = ?\n",
    "        \"\"\", [lattice_id]).fetchone()[0]\n",
    "        \n",
    "        edge_count = storage.conn.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM lattice_edges WHERE lattice_id = ?\n",
    "        \"\"\", [lattice_id]).fetchone()[0]\n",
    "        \n",
    "        print(f\"    ‚Ä¢ {lattice_type} lattice: {node_count} nodes, {edge_count} edges, dim={dimension}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n‚úÖ Unified storage successfully handles multiple perceptrons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5c6e6",
   "metadata": {},
   "source": [
    "## 7. Query Examples: Cross-Lattice Analysis\n",
    "\n",
    "Demonstrate querying across different lattice types and perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cfae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Top 10 token transitions (AM lattice)\n",
      "------------------------------------------------------------\n",
      "  quick ‚Üí brown: 1x\n",
      "  lazy ‚Üí dog: 1x\n",
      "  runs ‚Üí fast: 1x\n",
      "  the ‚Üí fox: 1x\n",
      "  fox ‚Üí jumps: 1x\n",
      "  the ‚Üí lazy: 1x\n",
      "  jumps ‚Üí over: 1x\n",
      "  the ‚Üí quick: 1x\n",
      "  fox ‚Üí runs: 1x\n",
      "  over ‚Üí the: 1x\n",
      "\n",
      "Query 2: Most similar context pairs (W lattice)\n",
      "------------------------------------------------------------\n",
      "  group_3 ‚Üî group_4: 0.250\n",
      "  group_0 ‚Üî group_1: 0.200\n",
      "  group_2 ‚Üî group_3: 0.200\n",
      "  group_1 ‚Üî group_2: 0.200\n",
      "\n",
      "Query 3: Largest tables (metadata lattice)\n",
      "------------------------------------------------------------\n",
      "  order_items: 150,000 rows\n",
      "  orders: 50,000 rows\n",
      "  customers: 10,000 rows\n",
      "  products: 1,000 rows\n",
      "\n",
      "Query 4: Tables with most foreign key relationships\n",
      "------------------------------------------------------------\n",
      "  order_items: 2 outgoing FK(s)\n",
      "  orders: 1 outgoing FK(s)\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Find most frequent transitions in AM\n",
    "print(\"Query 1: Top 10 token transitions (AM lattice)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "top_transitions = storage.conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        src.properties->>'token' as source_token,\n",
    "        tgt.properties->>'token' as target_token,\n",
    "        e.weight as frequency\n",
    "    FROM lattice_edges e\n",
    "    JOIN lattice_nodes src ON e.source_node = src.node_id\n",
    "    JOIN lattice_nodes tgt ON e.target_node = tgt.node_id\n",
    "    WHERE e.edge_type = 'am_transition'\n",
    "    ORDER BY e.weight DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for src, tgt, freq in top_transitions:\n",
    "    print(f\"  {src} ‚Üí {tgt}: {int(freq)}x\")\n",
    "\n",
    "# Query 2: Find most similar HLLSet pairs in W\n",
    "print(\"\\nQuery 2: Most similar context pairs (W lattice)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "similar_contexts = storage.conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        src.properties->>'position' as pos1,\n",
    "        tgt.properties->>'position' as pos2,\n",
    "        e.weight as similarity\n",
    "    FROM lattice_edges e\n",
    "    JOIN lattice_nodes src ON e.source_node = src.node_id\n",
    "    JOIN lattice_nodes tgt ON e.target_node = tgt.node_id\n",
    "    WHERE e.edge_type = 'w_morphism'\n",
    "    ORDER BY e.weight DESC\n",
    "    LIMIT 5\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for pos1, pos2, sim in similar_contexts:\n",
    "    print(f\"  {pos1} ‚Üî {pos2}: {sim:.3f}\")\n",
    "\n",
    "# Query 3: Find largest tables in metadata\n",
    "print(\"\\nQuery 3: Largest tables (metadata lattice)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "largest_tables = storage.conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        properties->>'table_name' as table_name,\n",
    "        CAST(properties->>'row_count' AS INTEGER) as row_count\n",
    "    FROM lattice_nodes\n",
    "    WHERE node_type = 'meta_table'\n",
    "    ORDER BY CAST(properties->>'row_count' AS INTEGER) DESC\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for table, rows in largest_tables:\n",
    "    print(f\"  {table}: {rows:,} rows\")\n",
    "\n",
    "# Query 4: Find tables with most dependencies\n",
    "print(\"\\nQuery 4: Tables with most foreign key relationships\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "table_dependencies = storage.conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        n.properties->>'table_name' as table_name,\n",
    "        COUNT(e.edge_id) as fk_count\n",
    "    FROM lattice_nodes n\n",
    "    LEFT JOIN lattice_edges e ON n.node_id = e.source_node\n",
    "    WHERE n.node_type = 'meta_table' AND e.edge_type = 'meta_fk'\n",
    "    GROUP BY n.node_id, n.properties\n",
    "    ORDER BY COUNT(e.edge_id) DESC\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for table, fk_count in table_dependencies:\n",
    "    print(f\"  {table}: {fk_count} outgoing FK(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b2194",
   "metadata": {},
   "source": [
    "## 8. Final Statistics\n",
    "\n",
    "Complete storage summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97798c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  UNIFIED STORAGE EXTENSION - FINAL STATISTICS\n",
      "======================================================================\n",
      "\n",
      "üìä Storage Contents:\n",
      "   ‚Ä¢ Perceptrons:        2\n",
      "   ‚Ä¢ Lattices:           3\n",
      "   ‚Ä¢ Lattice Nodes:      19\n",
      "   ‚Ä¢ Lattice Edges:      19\n",
      "   ‚Ä¢ HLLSets (unique):   5\n",
      "\n",
      "üíæ Compression Efficiency:\n",
      "   ‚Ä¢ Original Size:      81,920 bytes (80.0 KB)\n",
      "   ‚Ä¢ Compressed Size:    180 bytes (0.2 KB)\n",
      "   ‚Ä¢ Average Ratio:      461.80x\n",
      "   ‚Ä¢ Space Saved:        99.8%\n",
      "\n",
      "‚úÖ Unified storage successfully demonstrated!\n",
      "   All lattice types (AM, W, metadata) stored in unified schema\n",
      "   Multiple perceptrons with independent hash morphisms\n",
      "   Roaring compression achieving 461.8x reduction\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üîí Storage closed\n"
     ]
    }
   ],
   "source": [
    "final_stats = storage.get_storage_stats()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  UNIFIED STORAGE EXTENSION - FINAL STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Storage Contents:\")\n",
    "print(f\"   ‚Ä¢ Perceptrons:        {final_stats['perceptrons']}\")\n",
    "print(f\"   ‚Ä¢ Lattices:           {final_stats['lattices']}\")\n",
    "print(f\"   ‚Ä¢ Lattice Nodes:      {final_stats['nodes']}\")\n",
    "print(f\"   ‚Ä¢ Lattice Edges:      {final_stats['edges']}\")\n",
    "print(f\"   ‚Ä¢ HLLSets (unique):   {final_stats['hllsets']}\")\n",
    "\n",
    "comp = final_stats['hllset_compression']\n",
    "if comp['total_original'] > 0:\n",
    "    savings_pct = (1 - comp['total_compressed'] / comp['total_original']) * 100\n",
    "    print(f\"\\nüíæ Compression Efficiency:\")\n",
    "    print(f\"   ‚Ä¢ Original Size:      {comp['total_original']:,} bytes ({comp['total_original']/1024:.1f} KB)\")\n",
    "    print(f\"   ‚Ä¢ Compressed Size:    {comp['total_compressed']:,} bytes ({comp['total_compressed']/1024:.1f} KB)\")\n",
    "    print(f\"   ‚Ä¢ Average Ratio:      {comp['avg_compression_ratio']:.2f}x\")\n",
    "    print(f\"   ‚Ä¢ Space Saved:        {savings_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Unified storage successfully demonstrated!\")\n",
    "print(f\"   All lattice types (AM, W, metadata) stored in unified schema\")\n",
    "print(f\"   Multiple perceptrons with independent hash morphisms\")\n",
    "print(f\"   Roaring compression achieving {comp['avg_compression_ratio']:.1f}x reduction\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Close storage\n",
    "storage.close()\n",
    "print(\"\\nüîí Storage closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d612d4f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Unified Schema**: All lattice types (AM, W, metadata) stored in same tables with type discrimination\n",
    "2. **Multi-Perceptron Support**: Different perceptrons with independent configurations sharing storage\n",
    "3. **Roaring Compression**: 6-50x compression for HLLSets using bitmap encoding\n",
    "4. **Content-Addressable**: All artifacts identified by content hashes (IICA compliant)\n",
    "5. **Flexible Queries**: SQL queries across lattice types and perceptrons\n",
    "6. **Type Safety**: Node/edge types discriminate between different semantic meanings\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Entanglement Storage**: Cross-perceptron morphisms (œÜ: L‚ÇÅ ‚Üí L‚ÇÇ)\n",
    "- **Metadata Perceptron**: Full implementation for database schema processing\n",
    "- **Performance Testing**: Benchmark with large-scale data\n",
    "- **Migration Tools**: Convert existing storage to unified schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
